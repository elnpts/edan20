{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: A sentence embedder\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a sentence embedder simplified from Reimers and Gurevych's Sentence-BERT: https://arxiv.org/pdf/1908.10084. S-BERT is written in PyTorch and its code is available from GitHub: https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "The objectives of the assignment are to:\n",
    "* Write a program to embed sentences\n",
    "* Use neural networks with PyTorch\n",
    "* Write a short report of 2 to 3 pages to describe your program.\n",
    "\n",
    "Note: Should your machine be unable to train a model for the whole dataset, then use only a fraction of the dataset such as 10% or less. For this, use the `MINI_CORPUS` constant. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We saw we can vectorize words using a dense representation. We can extend this to documents. This enables us to store the resulting vectors in databases and then use fast algorithms for paragraph or document comparisons such as Faiss: https://github.com/facebookresearch/faiss\n",
    "\n",
    "There are many document vectorization techniques and models are regularly benchmarked, see: https://huggingface.co/spaces/mteb/leaderboard. See also a list of available vector databases here https://db-engines.com/en/ranking/vector+dbms\n",
    "\n",
    "In this lab, you will program two techniques to vectorize documents into dense vectors. You will first implement a baseline technique and then a toy version of SBERT. SBERT is one of the earliest transformer-based document vectorization algorithm: _Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks_ by Reimers and Gurevych (2019) https://arxiv.org/pdf/1908.10084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the algorithms\n",
    "Read the Getting Started paragraph of https://github.com/UKPLab/sentence-transformers for an overview.\n",
    "\n",
    "Read the summary of the SBERT paper as well as Sections 1 and 3, _Introduction_ and _Model_. In the triplet objective function, an anchor is a start sample, the positive sample is close to the anchor, while the negative one is different. Considering a language detector, think of a sentence in Swedish as the anchor. A positive sample would be another sentence in Swedish and a negative one could be a sentence in English.\n",
    "\n",
    "In the _Method and program struture_ section of your report, you will summarize these sections in 10 to 15 lines. Note that a three-way softmax classifier is simply a logistic regression with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import regex as re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a reduced dataset for the development of your program with `MINI_CORPUS` set to `True`. Once your program is ready, you can train your model on the whole dataset (if you have the time). Set `MINI_CORPUS` to `False` then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_CORPUS = True  # Set the value to True when you develop the program\n",
    "MINI_PERCENTAGE = 0.01  # Percentage of the original dataset.\n",
    "# Depending on your machine, you may even use less than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: SNLI\n",
    "As dataset, you will use SNLI. SNLI consists of over 500,000 lines with the text of the pairs and their labels. The authors created the dataset by giving volunteers a sentence (the premise) and asking them to write a second sentence (the hypothesis) that is either definitely true\n",
    "(entailment), that might be true (neutral), or that is definitely false (contradiction).\n",
    "\n",
    "Read the dataset description from this URL https://nlp.stanford.edu/projects/snli/ and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adjust the path to fit your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../tentan2023/snli_1.0/snli_1.0_train.jsonl', 'r') as f:\n",
    "    dataset_list = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = []\n",
    "for json_str in dataset_list:\n",
    "    dataset_snli += [json.loads(json_str)]\n",
    "    # print(f\"result: {result}\")\n",
    "    # print(isinstance(result, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with an agreement in the annotation. The final annotation is the gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['entailment'],\n",
       " 'captionID': '3706019259.jpg#3',\n",
       " 'gold_label': 'entailment',\n",
       " 'pairID': '3706019259.jpg#3r2e',\n",
       " 'sentence1': 'A foreign family is walking along a dirt path next to the water.',\n",
       " 'sentence1_binary_parse': '( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))',\n",
       " 'sentence2': 'A family of foreigners walks by the water.',\n",
       " 'sentence2_binary_parse': '( ( ( A family ) ( of foreigners ) ) ( ( walks ( by ( the water ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN of) (NP (NNS foreigners)))) (VP (VBZ walks) (PP (IN by) (NP (DT the) (NN water)))) (. .)))'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with no agreement in the annotation. The gold label is `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['contradiction', 'contradiction', 'neutral', 'neutral'],\n",
       " 'captionID': '2677109430.jpg#2',\n",
       " 'gold_label': '-',\n",
       " 'pairID': '2677109430.jpg#2r1c',\n",
       " 'sentence1': 'A small group of church-goers watch a choir practice.',\n",
       " 'sentence1_binary_parse': '( ( ( A ( small group ) ) ( of church-goers ) ) ( ( watch ( a ( choir practice ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (JJ small) (NN group)) (PP (IN of) (NP (NNS church-goers)))) (VP (VBP watch) (NP (DT a) (NN choir) (NN practice))) (. .)))',\n",
       " 'sentence2': 'A choir performs in front of packed crowd.',\n",
       " 'sentence2_binary_parse': '( ( A choir ) ( ( performs ( in ( front ( of ( packed crowd ) ) ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (DT A) (NN choir)) (VP (VBZ performs) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ packed) (NN crowd)))))) (. .)))'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the samples that have no agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = []\n",
    "for sample in dataset_snli:\n",
    "    s1 = sample['sentence1']\n",
    "    s2 = sample['sentence2']\n",
    "    label = sample['gold_label']\n",
    "    if label != '-':\n",
    "        dataset_str += [(s1, s2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is at a diner, ordering an omelette.',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is outdoors, on a horse.',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_CORPUS:\n",
    "    new_size = int(len(dataset_str) * MINI_PERCENTAGE)\n",
    "    dataset_str = dataset_str[:new_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: GloVe\n",
    "\n",
    "You will first implement a baseline, an easy technique that serves as comparison for more elaborate ones. \n",
    "\n",
    "In Sect. 4.1 and Table 1 the authors proposed a baseline technique for computing a semantic\n",
    "textual similarity between two sentences that uses GloVe embeddings. Describe this technique in 5 to 10 lines in the _Method and program struture_ section. You will create a *Baseline* subsection for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "You will use a list of pretrained word embeddings to implement the baseline and GloVe is one such vector lists. GloVe is available in different dimensionalities (50, 100, 200, 300) and vocabulary sizes (400,000 words, 1.2M, 2.4M). \n",
    "\n",
    "Download the GloVe 6B embeddings from https://nlp.stanford.edu/projects/glove/, uncompress it, and keep the `glove.6B.50d.txt` file of 400,000 words with 50-dimensional vectors.\n",
    "\n",
    "Please adjust your path to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '../../corpus/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor(\n",
    "            list(map(float, values[1:])), dtype=torch.float32)\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = next(iter(embeddings.values())).size()[0]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the words in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "glove = []\n",
    "for word, vector in embeddings.items():\n",
    "    glove_words += [word]\n",
    "    glove += [vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a tensor with the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.stack(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of word embeddings (400,000) and their dimension (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = glove.size()[1]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserve three special symbols: padding, unknown, and the first classification token of BERT. See the lecture on transformers for a clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['[PAD]', '[UNK]', '[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', 'the', ',', '.', 'of', 'to', 'and', 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = specials + glove_words\n",
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the vectors for the special tokens to our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.vstack((torch.zeros((3, d_model)), glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400003, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenization\n",
    "You will now tokenize the sentences\n",
    "\n",
    "Write a regular expression that tokenizes the words, numbers, and punctuation. Use Unicode classes. Note that a punctuation is a single symbol while the words and numbers are sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "pattern = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tokenization function that takes a string as input and results a list of tokens. Set the string in lower case by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize(sentence, pattern, lc=True):\n",
    "    if lc:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(dataset_str[0][0], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code that, for each sample of your dataset, builds a triple consisting of:\n",
    "1. The first tokenized sentence, \n",
    "2. The second one, and \n",
    "3. The class\n",
    "\n",
    "Build a list of all these triples to represent your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset_tokens = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build token-to-index `token2idx` and index-to-token `idx2token` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['the'], token2idx['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token[3], idx2token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the set of all the labels (classes) from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "labels = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'contradiction', 'neutral']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build label-to-index `label2idx` and index-to-label `idx2label` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'contradiction': 1, 'neutral': 2}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'contradiction', 2: 'neutral'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to convert:\n",
    "  * A list of tokens into a list of `LongTensor` indices and \n",
    "  * The class to a tensor. \n",
    "\n",
    "Your function should be able to handle two types: either a list or a string. The tokens are strored in a list and the class (label) is a string\n",
    "\n",
    "Note that an unknown token in GloVe should be mapped to the `UNK` symbol of index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def convert_symbols(symbols, symbol2idx):\n",
    "    if type(symbols) is str:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a list of tokens into a `LongTensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a label string into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][2], label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown tokens have the index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'person', 'on', 'a', 'horsewww']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('a person on a horsewww', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 902,  16,  10,   1,   1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(tokenize('a person on a horsewww wxwx', pattern), token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tokens and labels in your dataset by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([    10,    902,     17,     25,     10,  19304,      4,   7490,     32,\n",
       "         119031,      5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Embedding` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the GloVe vectors in a PyTorch `Embedding` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embs = nn.Embedding(glove.size()[0],\n",
    "                          glove.size()[1],\n",
    "                          padding_idx=0).from_pretrained(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the embedding for _the_ with its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embs(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Mean of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_str = dataset_str[0][0]\n",
    "s2_str = dataset_str[0][1]\n",
    "s3_str = dataset_str[0][2]\n",
    "s1_str, s2_str, s3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_idx = dataset[0][0]\n",
    "s2_idx = dataset[0][1]\n",
    "s3_idx = dataset[0][2]\n",
    "s1_idx, s2_idx, s3_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of indices and the GloVe embeddings as input and that computes the mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def mean_embs(input_idx: torch.LongTensor, glove_embs: nn.Embedding) -> torch.tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "         0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "        -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "         0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "        -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "        -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "         0.2139, -0.1993])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embs(dataset[0][0], glove_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the cosine of two vectors. You will return `torch.tensor(0.0)` if one of the vectors is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_cosine(v1: torch.tensor, v2: torch.tensor) -> torch.tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "          0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "         -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "          0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "         -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "         -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "          0.2139, -0.1993]),\n",
       " tensor([ 1.0878e-01,  3.7231e-01, -4.7114e-01, -1.3591e-02,  5.1340e-01,\n",
       "          3.7193e-01, -4.4592e-01, -5.9900e-02,  2.5352e-01, -1.3076e-01,\n",
       "          1.6231e-01,  2.3146e-03, -3.6474e-01,  2.3840e-03,  3.4653e-01,\n",
       "         -2.1769e-01,  2.5946e-02,  3.9537e-01, -6.9517e-01, -3.4811e-01,\n",
       "         -4.9454e-02,  1.4977e-01, -1.2447e-01,  8.1851e-02,  6.2581e-02,\n",
       "         -1.8692e+00, -3.1502e-01, -7.4079e-02,  1.2478e-01, -1.6717e-02,\n",
       "          3.3707e+00,  8.7725e-02, -4.0180e-01, -1.8131e-01,  2.5315e-01,\n",
       "          1.7589e-01,  2.2877e-01,  4.3286e-01, -1.6315e-02, -3.6988e-01,\n",
       "         -2.8208e-02,  3.1538e-02, -2.4721e-01,  2.5963e-01,  1.3792e-02,\n",
       "         -2.6431e-01,  1.7081e-02, -2.0042e-01,  1.6257e-01,  2.1471e-01]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = mean_embs(s1_idx, glove_embs)\n",
    "v2 = mean_embs(s2_idx, glove_embs)\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9426)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the cosine of pairs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:00<00:00, 40961.98it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        mean_embs(data[0], glove_embs),\n",
    "        mean_embs(data[1], glove_embs))\n",
    "    class_name = idx2label[data[2].item()]\n",
    "    cos_sim[class_name] += cos_val\n",
    "    cnt[class_name] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will comment these values in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.9453),\n",
       " 'neutral': tensor(0.9385),\n",
       " 'contradiction': tensor(0.9298)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT: The Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the SBERT architecture and replicate the pipeline in Fig. 1 in the paper. In the next cells, we walk through the figure from the bottom to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer builds an input consisting of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,  360, 5453]), tensor([   3,  194,  368, 2929]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the small cat', pattern))))\n",
    "p2 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the very big dog', pattern))))\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the BERT layer. Using PyTorch classes, create an encoder of four layers where each layer has five heads. You will use the classes `TransformerEncoderLayer` and `TransformerEncoder`. The dimensionality `d_model` is 50 as this is the size of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "      (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a BERT encoder. We associate each input index to an embedding. In the next cell, we create three embedding vectors correponding to three words.\n",
    "\n",
    "In the rest of the program, all our batches will have only one sample to eliminate the need for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9387, 0.6067, 0.6622, 0.8363, 0.9667, 0.6965, 0.1370, 0.4145, 0.3787,\n",
       "         0.4844, 0.2749, 0.7090, 0.3826, 0.8153, 0.5300, 0.2403, 0.4013, 0.3560,\n",
       "         0.1821, 0.1712, 0.2727, 0.8165, 0.8252, 0.2489, 0.9062, 0.5284, 0.2478,\n",
       "         0.7386, 0.6570, 0.9061, 0.8268, 0.7636, 0.6852, 0.9501, 0.4723, 0.4098,\n",
       "         0.6909, 0.7936, 0.0857, 0.3970, 0.1230, 0.3518, 0.5806, 0.1063, 0.4566,\n",
       "         0.8374, 0.4237, 0.9551, 0.7070, 0.8194],\n",
       "        [0.8793, 0.8419, 0.4827, 0.1530, 0.4276, 0.8951, 0.5334, 0.4928, 0.0532,\n",
       "         0.0369, 0.2521, 0.9173, 0.2706, 0.4892, 0.0833, 0.7840, 0.8600, 0.0076,\n",
       "         0.2423, 0.3238, 0.8596, 0.8776, 0.4359, 0.0461, 0.5570, 0.8471, 0.1270,\n",
       "         0.8806, 0.4139, 0.0888, 0.0696, 0.5807, 0.7729, 0.3495, 0.1469, 0.8590,\n",
       "         0.7776, 0.8736, 0.0180, 0.2255, 0.8880, 0.3719, 0.5252, 0.2080, 0.0197,\n",
       "         0.5450, 0.6084, 0.3629, 0.4917, 0.1790],\n",
       "        [0.0155, 0.0402, 0.7692, 0.6731, 0.7904, 0.4465, 0.7975, 0.4453, 0.6777,\n",
       "         0.1642, 0.8540, 0.1827, 0.0603, 0.0532, 0.2453, 0.5992, 0.9484, 0.8953,\n",
       "         0.7930, 0.7792, 0.8756, 0.0174, 0.4792, 0.5726, 0.5861, 0.7115, 0.4991,\n",
       "         0.9645, 0.8538, 0.3257, 0.6943, 0.0664, 0.4647, 0.8035, 0.9890, 0.5091,\n",
       "         0.9365, 0.0157, 0.5677, 0.5205, 0.1906, 0.1099, 0.7263, 0.2861, 0.7729,\n",
       "         0.3008, 0.9549, 0.5246, 0.3286, 0.5294]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(3, d_model)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it to the encoder to encode the input into three vectors of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2113, -2.3170,  1.4399, -1.0647, -0.9602, -0.3614, -0.9183,  1.1950,\n",
       "          0.3150,  1.2725, -0.7470,  2.1498,  0.9332, -0.1515, -0.7672,  0.4414,\n",
       "         -0.1168, -0.6752, -0.0058, -1.2873, -0.2411,  0.6451, -0.5454, -0.4517,\n",
       "          0.3685,  0.1368,  0.5351,  1.7200, -0.6264,  0.7756,  0.0543,  1.2888,\n",
       "         -1.2873,  1.2884,  1.4843,  1.2303,  0.1768, -0.1632, -0.8127,  0.9746,\n",
       "         -1.2001, -1.3049,  0.0394, -1.2753, -0.9777, -1.2118, -0.4672, -0.1477,\n",
       "         -0.2039,  2.0351],\n",
       "        [-0.0670, -1.4159,  0.1814, -1.2351, -0.8322,  0.5486,  0.3860,  0.8518,\n",
       "          0.1481,  0.4041,  0.7595,  1.2844,  1.3003, -1.0126, -0.8169,  0.4979,\n",
       "         -0.4529, -1.7831,  0.5050, -0.8598,  0.2140,  0.1058, -0.6845, -1.3228,\n",
       "          0.0330,  0.5616,  0.8534,  1.8224, -0.2070,  0.1194,  0.0117,  0.0637,\n",
       "         -1.5958,  1.3357,  1.2283,  2.7724,  0.5906, -0.0117, -0.2650,  1.1608,\n",
       "         -0.5117, -0.5833,  0.6873, -1.4961, -2.2832, -1.0147, -0.4330, -0.6459,\n",
       "         -0.2788,  1.3816],\n",
       "        [-1.3956, -1.5195,  0.3246, -0.5383, -1.0687, -0.6915,  1.1672,  0.8834,\n",
       "          1.1026, -0.4915,  0.8024, -0.4042,  0.1364, -0.6601, -0.5882,  0.7224,\n",
       "          0.0540, -1.2685,  0.8271,  0.2224, -0.2014, -1.6610,  0.1570, -0.0108,\n",
       "          0.1923,  0.1832,  1.9080,  2.5724,  0.0420,  0.7181, -0.3551,  0.5066,\n",
       "         -1.3248,  1.3960,  1.1889,  1.0868,  0.1779, -1.5605,  0.2817,  0.5982,\n",
       "         -1.4248, -1.9051,  0.3159, -0.6452, -1.2502, -1.5448,  0.8025,  0.5389,\n",
       "          0.6322,  0.9686]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a statement that will compute the mean of these embeddings. You will use the `mean(dim)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5579, -1.7508,  0.6486, -0.9460, -0.9537, -0.1681,  0.2116,  0.9767,\n",
       "         0.5219,  0.3951,  0.2717,  1.0100,  0.7900, -0.6081, -0.7241,  0.5539,\n",
       "        -0.1719, -1.2422,  0.4421, -0.6416, -0.0761, -0.3034, -0.3576, -0.5951,\n",
       "         0.1980,  0.2939,  1.0988,  2.0383, -0.2638,  0.5377, -0.0964,  0.6197,\n",
       "        -1.4026,  1.3400,  1.3005,  1.6965,  0.3151, -0.5785, -0.2653,  0.9112,\n",
       "        -1.0456, -1.2644,  0.3476, -1.1389, -1.5037, -1.2571, -0.0326, -0.0849,\n",
       "         0.0498,  1.4618], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have understood the first steps of SBERT, we can implement them in a class.\n",
    "\n",
    "In the next cell, write the `forward()` method that takes the two sentences as input in the form of two `LongTensor` of indices.\n",
    "1. Extract their embeddings from the GloVe embedding matrix.\n",
    "2. Encode them with the transformer, and \n",
    "3. Compute their respective mean. You will call these vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "4. Return these two vectors of means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(\n",
    "            glove.size()[0],\n",
    "            glove.size()[1],\n",
    "            padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        # We do not use this last line for now\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        ...\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0871,  0.6838, -0.8779,  1.8501,  1.2694, -0.3537, -0.0906,  0.6309,\n",
       "          0.7339, -0.1057, -1.8488, -0.7359,  0.1217,  2.0048,  0.9129,  0.4650,\n",
       "          0.2613,  0.7492, -0.5125, -0.6423, -0.1444, -0.0057, -1.4086, -0.2774,\n",
       "          0.5499, -1.3598, -2.9142,  0.2186,  0.1461,  0.2754,  0.7382,  1.0510,\n",
       "         -1.4944, -1.5059,  0.2656,  0.3182,  0.2919, -0.0440,  0.6076,  0.0616,\n",
       "         -0.3832,  0.1029,  0.0271,  1.1058, -1.4657, -0.0856,  0.1479,  1.7466,\n",
       "         -1.0182, -0.1500], grad_fn=<MeanBackward1>),\n",
       " tensor([ 0.0703,  1.1420, -0.9480,  1.4268,  1.0973, -0.0372, -0.6011,  0.7883,\n",
       "          1.0451, -0.4386, -1.1938, -0.7504, -0.0642,  1.5395,  0.3466,  0.2765,\n",
       "          0.2041,  1.2671, -0.4811, -0.2947, -0.0541, -0.2606, -1.2846,  0.1704,\n",
       "          0.1479, -1.5162, -2.8798,  0.0755,  0.2832,  0.2621,  1.1614,  0.8980,\n",
       "         -1.9583, -1.5787,  0.3846,  0.2978,  0.4014,  0.2166,  0.8796,  0.5418,\n",
       "         -0.9470,  0.1110, -0.0931,  1.1705, -1.2751, -0.1506, -0.4914,  2.0931,\n",
       "         -0.9664, -0.0336], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that concatenate `u`, `v`, and `|u-v|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0871,  0.6838, -0.8779,  1.8501,  1.2694, -0.3537, -0.0906,  0.6309,\n",
       "         0.7339, -0.1057, -1.8488, -0.7359,  0.1217,  2.0048,  0.9129,  0.4650,\n",
       "         0.2613,  0.7492, -0.5125, -0.6423, -0.1444, -0.0057, -1.4086, -0.2774,\n",
       "         0.5499, -1.3598, -2.9142,  0.2186,  0.1461,  0.2754,  0.7382,  1.0510,\n",
       "        -1.4944, -1.5059,  0.2656,  0.3182,  0.2919, -0.0440,  0.6076,  0.0616,\n",
       "        -0.3832,  0.1029,  0.0271,  1.1058, -1.4657, -0.0856,  0.1479,  1.7466,\n",
       "        -1.0182, -0.1500,  0.0703,  1.1420, -0.9480,  1.4268,  1.0973, -0.0372,\n",
       "        -0.6011,  0.7883,  1.0451, -0.4386, -1.1938, -0.7504, -0.0642,  1.5395,\n",
       "         0.3466,  0.2765,  0.2041,  1.2671, -0.4811, -0.2947, -0.0541, -0.2606,\n",
       "        -1.2846,  0.1704,  0.1479, -1.5162, -2.8798,  0.0755,  0.2832,  0.2621,\n",
       "         1.1614,  0.8980, -1.9583, -1.5787,  0.3846,  0.2978,  0.4014,  0.2166,\n",
       "         0.8796,  0.5418, -0.9470,  0.1110, -0.0931,  1.1705, -1.2751, -0.1506,\n",
       "        -0.4914,  2.0931, -0.9664, -0.0336,  0.0168,  0.4581,  0.0700,  0.4233,\n",
       "         0.1721,  0.3165,  0.5105,  0.1573,  0.3112,  0.3329,  0.6550,  0.0145,\n",
       "         0.1859,  0.4654,  0.5662,  0.1885,  0.0571,  0.5179,  0.0314,  0.3476,\n",
       "         0.0902,  0.2549,  0.1241,  0.4479,  0.4019,  0.1564,  0.0344,  0.1431,\n",
       "         0.1371,  0.0133,  0.4232,  0.1530,  0.4638,  0.0728,  0.1191,  0.0204,\n",
       "         0.1095,  0.2606,  0.2720,  0.4802,  0.5639,  0.0081,  0.1202,  0.0647,\n",
       "         0.1906,  0.0650,  0.6393,  0.3465,  0.0517,  0.1164],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to add the logistic regression head. Complement the `forward()` method so that it concatenates, $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{|u - v|}$ and outputs three classes. You have only two lines to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward() method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(glove.size()[0],\n",
    "                                       glove.size()[1],\n",
    "                                       padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        ...\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the complete model and we have three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2157,  0.1609, -0.3511], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SBERT\n",
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(sbert.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training loop. You will process one sample at a time to simplify it i.e. no batch. Record the training loss.\n",
    "\n",
    "A better design would use a `Dataset` object. We will see this construct in the last laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:38<00:00, 141.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.064804803865563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:36<00:00, 148.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987888264110532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:34<00:00, 161.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9480911479927512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:33<00:00, 161.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9125173025019027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:35<00:00, 156.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8803189503367744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHFCAYAAADmGm0KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAM0lEQVR4nO3de1hVZf7//9eWs4aEoYhBHErNc4qF4DBZGorp5IyVehVp01Q2miHjTJJZagcmK3NKofFApjVq5WFsNJMOoIZKOGCapM5HCiyI8KuAUqi4fn/wc+eOpbER2Gx8Pq5rXZf7Xvda6313X5/h9Vnr3mtbDMMwBAAAAButHF0AAABAc0RIAgAAMEFIAgAAMEFIAgAAMEFIAgAAMEFIAgAAMEFIAgAAMEFIAgAAMEFIAgAAMEFIAtDiLFu2TBaLRdnZ2Y4uBYATIyQBAACYICQBAACYICQBuCxt375dgwcPlre3t1q3bq2oqCht3LjRpk9lZaWmTZum0NBQeXp6ql27durfv79Wrlxp7XP48GGNHTtWnTp1koeHh/z9/TV48GDl5uY28YgANDRXRxcAAE0tIyNDt912m3r37q2lS5fKw8NDycnJGjlypFauXKkxY8ZIkhISErRixQo9++yz6tu3r06ePKl9+/bp6NGj1nMNHz5c1dXVmjt3rq655hqVlpYqMzNTx48fd9DoADQUi2EYhqOLAICGtGzZMt1///36/PPP1b9//1r7IyMjdfjwYf3f//2frrjiCklSdXW1brjhBh0/flwFBQWyWCzq1auXrrvuOq1bt870OkePHpWfn5/mz5+vxx57rFHHBKDp8bgNwGXl5MmT2rVrl+68805rQJIkFxcXxcXF6ciRIzpw4IAk6aabbtIHH3yg6dOnKz09XT/++KPNudq1a6drr71WL774oubNm6ecnBydPXu2SccDoPEQkgBcVo4dOybDMBQQEFBrX6dOnSTJ+jjt1Vdf1eOPP67169frlltuUbt27TRq1CgdOnRIkmSxWPTxxx9r6NChmjt3rvr166f27dtrypQpqqioaLpBAWgUhCQAlxVfX1+1atVKRUVFtfZ99913kiQ/Pz9JUps2bTR79mx99dVXKi4uVkpKinbu3KmRI0dajwkODtbSpUtVXFysAwcOaOrUqUpOTtZf//rXphkQgEZDSAJwWWnTpo0iIiK0du1am8dnZ8+e1VtvvaXAwEB16dKl1nH+/v6aMGGCxo0bpwMHDqiysrJWny5duujJJ59Ur1699N///rdRxwGg8fHtNgAt1ieffKKvv/66VntSUpJuu+023XLLLZo2bZrc3d2VnJysffv2aeXKlbJYLJKkiIgIjRgxQr1795avr6/y8vK0YsUKRUZGqnXr1vriiy80efJk3XXXXercubPc3d31ySef6IsvvtD06dObeLQAGhohCUCL9fjjj5u25+fn65NPPtHTTz+tCRMm6OzZs+rTp482bNigESNGWPvdeuut2rBhg1555RVVVlbq6quv1n333acZM2ZIkjp27Khrr71WycnJKiwslMViUVhYmF5++WU9+uijTTJGAI2HVwAAAACYYE0SAACACUISAACACUISAACACUISAACACUISAACACUISAACACd6TVE9nz57Vd999J29vb+uL5wAAQPNmGIYqKirUqVMntWp18XtFhKR6+u677xQUFOToMgAAQD0UFhYqMDDwon0ISfXk7e0tqeY/ctu2bR1cDQAAqIvy8nIFBQVZ/45fDCGpns49Ymvbti0hCQAAJ1OXpTIs3AYAADBBSAIAADBBSAIAADBBSAIAADBBSAIAADBBSAIAADBBSAIAADBBSAIAADBBSAIAADBBSGomysqkI0fM9x05UrMfAAA0HUJSM1BWJg0bJt18s1RYaLuvsLCmfdgwghIAAE2JkNQMVFRIJSXS4cPSoEE/B6XCwprPhw/X7K+ocGSVAABcXghJzUBgoJSeLoWF/RyUMjN/DkhhYTX7AwMdWycAAJcTV0cXgBpBQTVB6FwwGjiwpv1cQAoKcmBxAABchriT1IwEBUkrVti2rVhBQAIAwBEISc1IYaEUF2fbFhdXezE3AABofISkZuL8RdphYdJnn9muUSIoAQDQtAhJzcCRI7UXaUdF1V7MfaH3KAEAgIbHwu1mwNtb6tCh5t/nL9I+fzF3hw41/QAAQNMgJDUDPj7S5s0170H65df8g4KkjIyagOTj45j6AAC4HBGSmgkfnwuHIN6PBABA02NNEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAlCEgAAgAmHhqStW7dq5MiR6tSpkywWi9avX/+rx2RkZCg8PFyenp4KCwvT66+/brN/2bJlslgstbaffvrJpl9ycrJCQ0Pl6emp8PBwbdu2rSGHBgAAnJxDQ9LJkyfVp08fLViwoE798/PzNXz4cEVHRysnJ0dPPPGEpkyZojVr1tj0a9u2rYqKimw2T09P6/7Vq1crPj5eM2bMUE5OjqKjoxUbG6uCgoIGHR8AAHBeFsMwDEcXIUkWi0Xr1q3TqFGjLtjn8ccf14YNG5SXl2dtmzhxovbs2aMdO3ZIqrmTFB8fr+PHj1/wPBEREerXr59SUlKsbd26ddOoUaOUlJRUp3rLy8vl4+OjsrIytW3btk7HAAAAx7Ln77dTrUnasWOHYmJibNqGDh2q7OxsnT592tp24sQJBQcHKzAwUCNGjFBOTo5136lTp7R79+5a54mJiVFmZuYFr11VVaXy8nKbDQAAtFxOFZKKi4vl7+9v0+bv768zZ86otLRUknT99ddr2bJl2rBhg1auXClPT08NHDhQhw4dkiSVlpaqurra9DzFxcUXvHZSUpJ8fHysW1BQUAOPDgAANCdOFZKkmsdy5zv3tPBc+4ABA3TvvfeqT58+io6O1jvvvKMuXbrotdde+9Xz/LLtfImJiSorK7NuhYWFDTEcAADQTLk6ugB7dOzYsdbdnpKSErm6uuqqq64yPaZVq1a68cYbrXeS/Pz85OLiYnqeX95dOp+Hh4c8PDwucQQAAMBZONWdpMjISKWlpdm0bdmyRf3795ebm5vpMYZhKDc3VwEBAZIkd3d3hYeH1zpPWlqaoqKiGqdwAADgdBx6J+nEiRP63//+Z/2cn5+v3NxctWvXTtdcc40SExP17bffavny5ZJqvsm2YMECJSQk6MEHH9SOHTu0dOlSrVy50nqO2bNna8CAAercubPKy8v16quvKjc3VwsXLrT2SUhIUFxcnPr376/IyEgtWrRIBQUFmjhxYtMNHgAANGsODUnZ2dm65ZZbrJ8TEhIkSePHj9eyZctUVFRk8+6i0NBQbdq0SVOnTtXChQvVqVMnvfrqqxo9erS1z/Hjx/XQQw+puLhYPj4+6tu3r7Zu3aqbbrrJ2mfMmDE6evSo5syZo6KiIvXs2VObNm1ScHBwE4waAAA4g2bzniRnw3uSAABwPi32PUkAAABNhZAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABgwqEhaevWrRo5cqQ6deoki8Wi9evX/+oxGRkZCg8Pl6enp8LCwvT666/b7F+8eLGio6Pl6+srX19fDRkyRFlZWTZ9Zs2aJYvFYrN17NixIYcGAACcnEND0smTJ9WnTx8tWLCgTv3z8/M1fPhwRUdHKycnR0888YSmTJmiNWvWWPukp6dr3Lhx+vTTT7Vjxw5dc801iomJ0bfffmtzrh49eqioqMi67d27t0HHBgAAnJurIy8eGxur2NjYOvd//fXXdc0112j+/PmSpG7duik7O1svvfSSRo8eLUl6++23bY5ZvHix3nvvPX388ce67777rO2urq7cPQIAABfkVGuSduzYoZiYGJu2oUOHKjs7W6dPnzY9prKyUqdPn1a7du1s2g8dOqROnTopNDRUY8eO1eHDhy967aqqKpWXl9tsAACg5XKqkFRcXCx/f3+bNn9/f505c0alpaWmx0yfPl1XX321hgwZYm2LiIjQ8uXL9eGHH2rx4sUqLi5WVFSUjh49esFrJyUlycfHx7oFBQU1zKAAAECz5FQhSZIsFovNZ8MwTNslae7cuVq5cqXWrl0rT09Pa3tsbKxGjx6tXr16aciQIdq4caMk6c0337zgdRMTE1VWVmbdCgsLG2I4AACgmXLomiR7dezYUcXFxTZtJSUlcnV11VVXXWXT/tJLL+n555/XRx99pN69e1/0vG3atFGvXr106NChC/bx8PCQh4dH/YsHAABOxanuJEVGRiotLc2mbcuWLerfv7/c3NysbS+++KKeeeYZbd68Wf379//V81ZVVSkvL08BAQENXjMAAHBODg1JJ06cUG5urnJzcyXVfMU/NzdXBQUFkmoecZ3/jbSJEyfqm2++UUJCgvLy8pSamqqlS5dq2rRp1j5z587Vk08+qdTUVIWEhKi4uFjFxcU6ceKEtc+0adOUkZGh/Px87dq1S3feeafKy8s1fvz4phk4AABo9hwakrKzs9W3b1/17dtXkpSQkKC+ffvqqaeekiQVFRVZA5MkhYaGatOmTUpPT9cNN9ygZ555Rq+++qr16/+SlJycrFOnTunOO+9UQECAdXvppZesfY4cOaJx48apa9eu+sMf/iB3d3ft3LlTwcHBTTRyAADQ3FmMcyufYZfy8nL5+PiorKxMbdu2dXQ5AACgDuz5++1Ua5IAAACaCiEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADABCEJAADAhEND0tatWzVy5Eh16tRJFotF69ev/9VjMjIyFB4eLk9PT4WFhen111+v1WfNmjXq3r27PDw81L17d61bt65Wn+TkZIWGhsrT01Ph4eHatm1bQwwJAAC0EA4NSSdPnlSfPn20YMGCOvXPz8/X8OHDFR0drZycHD3xxBOaMmWK1qxZY+2zY8cOjRkzRnFxcdqzZ4/i4uJ09913a9euXdY+q1evVnx8vGbMmKGcnBxFR0crNjZWBQUFDT5GAADgnCyGYRiOLkKSLBaL1q1bp1GjRl2wz+OPP64NGzYoLy/P2jZx4kTt2bNHO3bskCSNGTNG5eXl+uCDD6x9hg0bJl9fX61cuVKSFBERoX79+iklJcXap1u3bho1apSSkpLqVG95ebl8fHxUVlamtm3b2jNUAADgIPb8/XaqNUk7duxQTEyMTdvQoUOVnZ2t06dPX7RPZmamJOnUqVPavXt3rT4xMTHWPmaqqqpUXl5uswEAgJbLqUJScXGx/P39bdr8/f115swZlZaWXrRPcXGxJKm0tFTV1dUX7WMmKSlJPj4+1i0oKKghhgQAAJoppwpJUs1jufOde1p4frtZn1+21aXP+RITE1VWVmbdCgsL61U/AABwDq6OLsAeHTt2rHW3p6SkRK6urrrqqqsu2ufcnSM/Pz+5uLhctI8ZDw8PeXh4NMQwAACAE7D7TtKECRO0devWxqjlV0VGRiotLc2mbcuWLerfv7/c3Nwu2icqKkqS5O7urvDw8Fp90tLSrH0AAADsDkkVFRWKiYlR586d9fzzz+vbb7+t98VPnDih3Nxc5ebmSqr5in9ubq71q/iJiYm67777rP0nTpyob775RgkJCcrLy1NqaqqWLl2qadOmWfs89thj2rJli1544QV99dVXeuGFF/TRRx8pPj7e2ichIUFLlixRamqq8vLyNHXqVBUUFGjixIn1HgsAAGhhjHooLS015s+fb9xwww2Gq6urMWzYMOPdd981Tp06Zdd5Pv30U0NSrW38+PGGYRjG+PHjjZtvvtnmmPT0dKNv376Gu7u7ERISYqSkpNQ677vvvmt07drVcHNzM66//npjzZo1tfosXLjQCA4ONtzd3Y1+/foZGRkZdtVeVlZmSDLKysrsOg4AADiOPX+/L/k9STk5OUpNTdWSJUt0xRVX6N5779Wf//xnde7c+ZIDXHPGe5IAAHA+TfaepKKiIm3ZskVbtmyRi4uLhg8fri+//FLdu3fXK6+8cimnBgAAcCi7Q9Lp06e1Zs0ajRgxQsHBwXr33Xc1depUFRUV6c0339SWLVu0YsUKzZkzpzHqBQAAaBJ2vwIgICBAZ8+e1bhx45SVlaUbbrihVp+hQ4fqyiuvbIDyAAAAHMPukPTKK6/orrvukqen5wX7+Pr6Kj8//5IKAwAAcCS7Q1JcXJz134WFhbJYLAoMDGzQogAAABzN7jVJZ86c0cyZM+Xj46OQkBAFBwfLx8dHTz75pPVHZgEAAJyd3XeSJk+erHXr1mnu3LmKjIyUJO3YsUOzZs1SaWmpXn/99QYvEgAAoKnZ/Z4kHx8frVq1SrGxsTbtH3zwgcaOHauysrIGLbC54j1JAAA4n0Z9T5Knp6dCQkJqtYeEhMjd3d3e0wEAADRLdoekSZMm6ZlnnlFVVZW1raqqSs8995wmT57coMUBAAA4it1rknJycvTxxx8rMDBQffr0kSTt2bNHp06d0uDBg/WHP/zB2nft2rUNVynQjJWVSRUVktkXPY8ckby9JR+fpq8LAFB/doekK6+8UqNHj7ZpCwoKarCCAGdTViYNGyaVlEjp6dL5/+dQWCgNGiR16CBt3kxQAgBnYndIeuONNxqjDsBpVVTUBKTDh2sC0bmgdC4gHT78cz9CEgA4j3r/wO0PP/yg7du367PPPtMPP/zQkDUBTiUwsCYYhYX9HJQyM38OSGFhNft55yoAOBe7Q9LJkyf1xz/+UQEBAfrtb3+r6OhoderUSQ888IAqKysbo0ag2QsKsg1KAwfaBiSeSAOA87E7JCUkJCgjI0Pvv/++jh8/ruPHj+vf//63MjIy9Je//KUxagScQlCQtGKFbduKFQQkAHBWdr9M0s/PT++9954GDRpk0/7pp5/q7rvvvmwevfEySfzSL9cgSdxJAoDmplFfJllZWSl/f/9a7R06dOBxGy5b5weksDDps89s1ygVFjq6QgCAvewOSZGRkXr66af1008/Wdt+/PFHzZ492/pbbsDl5MiR2ou0o6JqL+Y+csSxdQIA7GP3KwDmz5+v2NhY68skLRaLcnNz5enpqQ8//LAxagSaNW/vmvcgSbaP1s4t5j73niRvbwcVCACoF7vXJEk1d47eeustffXVVzIMQ927d9c999wjLy+vxqixWWJNEs7HG7cBwDnY8/fbrjtJp0+fVteuXfWf//xHDz744CUVCbQkPj4XDkG8HwkAnJNda5Lc3NxUVVUli8XSWPUAAAA0C3Yv3H700Uf1wgsv6MyZM41RDwAAQLNg98LtXbt26eOPP9aWLVvUq1cvtWnTxmb/2rVrG6w4AAAAR7E7JF155ZUaPXp0Y9QCAADQbNgdkt54443GqAMAAKBZsXtN0q233qrjx4/Xai8vL9ett97aEDUBAAA4nN0hKT09XadOnarV/tNPP2nbtm0NUhQAAICj1flx2xdffGH99/79+1VcXGz9XF1drc2bN+vqq69u2OoAAAAcpM4h6YYbbpDFYpHFYjF9rObl5aXXXnutQYsDAABwlDqHpPz8fBmGobCwMGVlZal9+/bWfe7u7urQoYNcXFwapUgAAICmVueQFBwcLEk6e/ZsoxUDAADQXNj9CgBJOnjwoNLT01VSUlIrND311FMNUhgAAIAj2f3ttsWLF6t79+566qmn9N5772ndunXWbf369XYXkJycrNDQUHl6eio8PPxXvyG3cOFCdevWTV5eXuratauWL19us3/QoEHWtVPnb7fffru1z6xZs2rt79ixo921AwCAlsvuO0nPPvusnnvuOT3++OOXfPHVq1crPj5eycnJGjhwoP75z38qNjZW+/fv1zXXXFOrf0pKihITE7V48WLdeOONysrK0oMPPihfX1+NHDlSUs3Popz/ioKjR4+qT58+uuuuu2zO1aNHD3300UfWz6ynAgAA57M7JB07dqxW4KivefPm6YEHHtCf/vQnSdL8+fP14YcfKiUlRUlJSbX6r1ixQg8//LDGjBkjSQoLC9POnTv1wgsvWENSu3btbI5ZtWqVWrduXatmV1dX7h4BAIALsvtx21133aUtW7Zc8oVPnTql3bt3KyYmxqY9JiZGmZmZpsdUVVXJ09PTps3Ly0tZWVk6ffq06TFLly7V2LFja/0Q76FDh9SpUyeFhoZq7NixOnz48EXrraqqUnl5uc0GAABaLrvvJF133XWaOXOmdu7cqV69esnNzc1m/5QpU+p0ntLSUlVXV8vf39+m3d/f3+ZFlecbOnSolixZolGjRqlfv37avXu3UlNTdfr0aZWWliogIMCmf1ZWlvbt26elS5fatEdERGj58uXq0qWLvv/+ez377LOKiorSl19+qauuusr02klJSZo9e3adxgYAAJyfxTAMw54DQkNDL3wyi+VX78ic89133+nqq69WZmamIiMjre3PPfecVqxYoa+++qrWMT/++KMmTZqkFStWyDAM+fv7695779XcuXP1/fffq0OHDjb9H374YWVmZmrv3r0XreXkyZO69tpr9be//U0JCQmmfaqqqlRVVWX9XF5erqCgIJWVlalt27Z1GjMAAHCs8vJy+fj41Onvt913kvLz8+td2Pn8/Pzk4uJS665RSUlJrbtL53h5eSk1NVX//Oc/9f333ysgIECLFi2St7e3/Pz8bPpWVlZq1apVmjNnzq/W0qZNG/Xq1UuHDh26YB8PDw95eHjUYWQAAKAlsHtN0jmnTp3SgQMHdObMmXod7+7urvDwcKWlpdm0p6WlKSoq6qLHurm5KTAwUC4uLlq1apVGjBihVq1sh/LOO++oqqpK995776/WUlVVpby8vFqP6wAAwOXL7pBUWVmpBx54QK1bt1aPHj1UUFAgqWYt0t///ne7zpWQkKAlS5YoNTVVeXl5mjp1qgoKCjRx4kRJUmJiou677z5r/4MHD+qtt97SoUOHlJWVpbFjx2rfvn16/vnna5176dKlGjVqlOkao2nTpikjI0P5+fnatWuX7rzzTpWXl2v8+PF21Q8AAFouu0NSYmKi9uzZo/T0dJtvmg0ZMkSrV6+261xjxozR/PnzNWfOHN1www3aunWrNm3aZP0JlKKiImsIk6Tq6mq9/PLL6tOnj2677Tb99NNPyszMVEhIiM15Dx48qO3bt+uBBx4wve6RI0c0btw4de3aVX/4wx/k7u6unTt3Wq8LAABg98Lt4OBgrV69WgMGDJC3t7f27NmjsLAw/e9//1O/fv0um6/G27PwCwAANA/2/P22+07SDz/8UOtbZFLNN8QsFou9pwMAAGiW7A5JN954ozZu3Gj9fC4YLV682Oar/AAAAM7M7lcAJCUladiwYdq/f7/OnDmjf/zjH/ryyy+1Y8cOZWRkNEaNAAAATc7uO0lRUVH67LPPVFlZqWuvvVZbtmyRv7+/duzYofDw8MaoEQAAoMnZvXAbNVi4DQCA82nUhdsAAACXA0ISAACACUISAACACUISAACAiUsOSeXl5Vq/fr3y8vIaoh4AAIBmwe6QdPfdd2vBggWSpB9//FH9+/fX3Xffrd69e2vNmjUNXiAAAIAj2B2Stm7dqujoaEnSunXrZBiGjh8/rldffVXPPvtsgxcIAADgCHaHpLKyMrVr106StHnzZo0ePVqtW7fW7bffrkOHDjV4gQAAAI5gd0gKCgrSjh07dPLkSW3evFkxMTGSpGPHjsnT07PBCwQAAHAEu3+7LT4+Xvfcc4+uuOIKBQcHa9CgQZJqHsP16tWroesDAABwCLtD0p///GfddNNNKiws1G233aZWrWpuRoWFhbEmCQAAtBiX/Ntt1dXV2rt3r4KDg+Xr69tQdTV7/HYbAADOp1F/uy0+Pl5Lly6VVBOQbr75ZvXr109BQUFKT0+vV8EAAADNjd0h6b333lOfPn0kSe+//77y8/P11VdfKT4+XjNmzGjwAgEAABzB7pBUWlqqjh07SpI2bdqku+66S126dNEDDzygvXv3NniBAAAAjmB3SPL399f+/ftVXV2tzZs3a8iQIZKkyspKubi4NHiBAAAAjmD3t9vuv/9+3X333QoICJDFYtFtt90mSdq1a5euv/76Bi8QAADAEewOSbNmzVLPnj1VWFiou+66Sx4eHpIkFxcXTZ8+vcELBAAAcIRLfgXA5YpXAAAA4Hwa9RUAkpSRkaGRI0fquuuuU+fOnfW73/1O27Ztq1exAAAAzZHdIemtt97SkCFD1Lp1a02ZMkWTJ0+Wl5eXBg8erH/961+NUSMAAECTs/txW7du3fTQQw9p6tSpNu3z5s3T4sWLlZeX16AFNlc8bgMAwPk06uO2w4cPa+TIkbXaf/e73yk/P9/e0wEAADRLdoekoKAgffzxx7XaP/74YwUFBTVIUQAAAI5m9ysA/vKXv2jKlCnKzc1VVFSULBaLtm/frmXLlukf//hHY9QIAADQ5OwOSY888og6duyol19+We+8846kmnVKq1ev1h133NHgBQIAADiCXSHpzJkzeu655/THP/5R27dvb6yaAAAAHM6uNUmurq568cUXVV1d3Vj1AAAANAt2L9weMmSI0tPTG6EUAACA5sPuNUmxsbFKTEzUvn37FB4erjZt2tjs/93vftdgxQEAADiK3XeSHnnkEX3//feaN2+e7rnnHo0aNcq6/f73v7e7gOTkZIWGhsrT01Ph4eG/+vMmCxcuVLdu3eTl5aWuXbtq+fLlNvuXLVsmi8VSa/vpp58u6boAAODyYvedpLNnzzbYxVevXq34+HglJydr4MCB+uc//6nY2Fjt379f11xzTa3+KSkpSkxM1OLFi3XjjTcqKytLDz74oHx9fW1ecNm2bVsdOHDA5lhPT896XxcAAFx+7P5ZkoYUERGhfv36KSUlxdrWrVs3jRo1SklJSbX6R0VFaeDAgXrxxRetbfHx8crOzrZ+227ZsmWKj4/X8ePHG+y6ZvhZEqBlKSuTKiqkwMDa+44ckby9JR+fpq8LQMNqlJ8l+eSTT9S9e3eVl5fX2ldWVqYePXpo69atdS7y1KlT2r17t2JiYmzaY2JilJmZaXpMVVWVzR0hSfLy8lJWVpZOnz5tbTtx4oSCg4MVGBioESNGKCcn55Kue+7a5eXlNhuAlqGsTBo2TLr5Zqmw0HZfYWFN+7BhNf0AXD7qHJLmz5+vBx980DR1+fj46OGHH9Yrr7xS5wuXlpaqurpa/v7+Nu3+/v4qLi42PWbo0KFasmSJdu/eLcMwlJ2drdTUVJ0+fVqlpaWSpOuvv17Lli3Thg0btHLlSnl6emrgwIE6dOhQva8rSUlJSfLx8bFu/AQL0HJUVEglJdLhw9KgQT8HpcLCms+HD9fsr6hwZJUAmlqdQ9KePXs0bNiwC+6PiYnR7t277S7AYrHYfDYMo1bbOTNnzlRsbKwGDBggNzc33XHHHZowYYIkycXFRZI0YMAA3XvvverTp4+io6P1zjvvqEuXLnrttdfqfV1JSkxMVFlZmXUr/OX/uwnAaQUGSunpUljYz0EpM/PngBQWVrPf7FEcgJarziHp+++/l5ub2wX3u7q66ocffqjzhf38/OTi4lLr7k1JSUmtuzzneHl5KTU1VZWVlfr6669VUFCgkJAQeXt7y8/Pz/SYVq1a6cYbb7TeSarPdSXJw8NDbdu2tdkAtBxBQbZBaeBA24DEzWPg8lPnkHT11Vdr7969F9z/xRdfKCAgoM4Xdnd3V3h4uNLS0mza09LSFBUVddFj3dzcFBgYKBcXF61atUojRoxQq1bmQzEMQ7m5udbaLuW6AFq2oCBpxQrbthUrCEjA5arOrwAYPny4nnrqKcXGxtZaPP3jjz/q6aef1ogRI+y6eEJCguLi4tS/f39FRkZq0aJFKigo0MSJEyXVPOL69ttvre9COnjwoLKyshQREaFjx45p3rx52rdvn958803rOWfPnq0BAwaoc+fOKi8v16uvvqrc3FwtXLiwztcFcHkqLJTi4mzb4uK4kwRcruockp588kmtXbtWXbp00eTJk9W1a1dZLBbl5eVp4cKFqq6u1owZM+y6+JgxY3T06FHNmTNHRUVF6tmzpzZt2qTg4GBJUlFRkQoKCqz9q6ur9fLLL+vAgQNyc3PTLbfcoszMTIWEhFj7HD9+XA899JCKi4vl4+Ojvn37auvWrbrpppvqfF0Al5/zF2mHhdXcQYqL+3mNEkEJuPzY9Z6kb775Ro888og+/PBDnTvMYrFo6NChSk5OtgkrLR3vSQJajiNHar7m/8s1SL8MThkZLN4GnJ09f7/teuN2cHCwNm3apGPHjul///ufDMNQ586d5evre0kFA4AjeXtLHTrU/Pv8O0bnFnMPGlSz39vbQQUCcAiHvnHbmXEnCWhZeOM2cHlotDtJANBS+fhcOATxiA24PNX5FQAAAACXE0ISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACYeHpOTkZIWGhsrT01Ph4eHatm3bRfsvXLhQ3bp1k5eXl7p27arly5fb7F+8eLGio6Pl6+srX19fDRkyRFlZWTZ9Zs2aJYvFYrN17NixwccGAACcl0ND0urVqxUfH68ZM2YoJydH0dHRio2NVUFBgWn/lJQUJSYmatasWfryyy81e/ZsTZo0Se+//761T3p6usaNG6dPP/1UO3bs0DXXXKOYmBh9++23Nufq0aOHioqKrNvevXsbdawAAMC5WAzDMBx18YiICPXr108pKSnWtm7dumnUqFFKSkqq1T8qKkoDBw7Uiy++aG2Lj49Xdna2tm/fbnqN6upq+fr6asGCBbrvvvsk1dxJWr9+vXJzc+tde3l5uXx8fFRWVqa2bdvW+zwAAKDp2PP322F3kk6dOqXdu3crJibGpj0mJkaZmZmmx1RVVcnT09OmzcvLS1lZWTp9+rTpMZWVlTp9+rTatWtn037o0CF16tRJoaGhGjt2rA4fPnwJowEAAC2Nw0JSaWmpqqur5e/vb9Pu7++v4uJi02OGDh2qJUuWaPfu3TIMQ9nZ2UpNTdXp06dVWlpqesz06dN19dVXa8iQIda2iIgILV++XB9++KEWL16s4uJiRUVF6ejRoxest6qqSuXl5TYbAABouRy+cNtisdh8NgyjVts5M2fOVGxsrAYMGCA3NzfdcccdmjBhgiTJxcWlVv+5c+dq5cqVWrt2rc0dqNjYWI0ePVq9evXSkCFDtHHjRknSm2++ecE6k5KS5OPjY92CgoLsHSoAAHAiDgtJfn5+cnFxqXXXqKSkpNbdpXO8vLyUmpqqyspKff311yooKFBISIi8vb3l5+dn0/ell17S888/ry1btqh3794XraVNmzbq1auXDh06dME+iYmJKisrs26FhYV1HCkAAHBGDgtJ7u7uCg8PV1pamk17WlqaoqKiLnqsm5ubAgMD5eLiolWrVmnEiBFq1ernobz44ot65plntHnzZvXv3/9Xa6mqqlJeXp4CAgIu2MfDw0Nt27a12QAAQMvl6siLJyQkKC4uTv3791dkZKQWLVqkgoICTZw4UVLN3Ztvv/3W+i6kgwcPKisrSxERETp27JjmzZunffv22Twmmzt3rmbOnKl//etfCgkJsd6puuKKK3TFFVdIkqZNm6aRI0fqmmuuUUlJiZ599lmVl5dr/PjxTfxfAAAANFcODUljxozR0aNHNWfOHBUVFalnz57atGmTgoODJUlFRUU270yqrq7Wyy+/rAMHDsjNzU233HKLMjMzFRISYu2TnJysU6dO6c4777S51tNPP61Zs2ZJko4cOaJx48aptLRU7du314ABA7Rz507rdQEAABz6niRnxnuSAABwPk7xniQAAIDmjJAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAABggpAEAGgRysqkI0fM9x05UrMfsAchCQDg9MrKpGHDpJtvlgoLbfcVFta0DxtGUIJ9CEkAAKdXUSGVlEiHD0uDBv0clAoLaz4fPlyzv6LCkVXC2RCSAABOLzBQSk+XwsJ+DkqZmT8HpLCwmv2BgY6tE87F1dEFAADQEIKCaoLQuWA0cGBN+7mAFBTkwOLglLiTBABoMYKCpBUrbNtWrCAgoX4ISQCAFqOwUIqLs22Li6u9mBuoC0ISAKBFOH+RdliY9NlntmuUCEqwFyEJAOD0jhypvUg7Kqr2Yu4LvUcJMMPCbQCA0/P2ljp0qPn3+Yu0z1/M3aFDTT+grghJAACn5+Mjbd5c8x6kX37NPyhIysioCUg+Po6pD86JkAQAaBF8fC4cgng/EuqDNUkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmCEkAAAAmHB6SkpOTFRoaKk9PT4WHh2vbtm0X7b9w4UJ169ZNXl5e6tq1q5YvX16rz5o1a9S9e3d5eHioe/fuWrdu3SVfFwAAXF4cGpJWr16t+Ph4zZgxQzk5OYqOjlZsbKwKCgpM+6ekpCgxMVGzZs3Sl19+qdmzZ2vSpEl6//33rX127NihMWPGKC4uTnv27FFcXJzuvvtu7dq1q97XBQAAlx+LYRiGoy4eERGhfv36KSUlxdrWrVs3jRo1SklJSbX6R0VFaeDAgXrxxRetbfHx8crOztb27dslSWPGjFF5ebk++OADa59hw4bJ19dXK1eurNd1zZSXl8vHx0dlZWVq27atfQMHAAAOYc/fb4fdSTp16pR2796tmJgYm/aYmBhlZmaaHlNVVSVPT0+bNi8vL2VlZen06dOSau4k/fKcQ4cOtZ6zPtc9d+3y8nKbDQAAtFwOC0mlpaWqrq6Wv7+/Tbu/v7+Ki4tNjxk6dKiWLFmi3bt3yzAMZWdnKzU1VadPn1Zpaakkqbi4+KLnrM91JSkpKUk+Pj7WLSgoyO4xAwAA5+HwhdsWi8Xms2EYtdrOmTlzpmJjYzVgwAC5ubnpjjvu0IQJEyRJLi4udp3TnutKUmJiosrKyqxbYWHhr44NAAA4L4eFJD8/P7m4uNS6e1NSUlLrLs85Xl5eSk1NVWVlpb7++msVFBQoJCRE3t7e8vPzkyR17Njxouesz3UlycPDQ23btrXZAABAy+WwkOTu7q7w8HClpaXZtKelpSkqKuqix7q5uSkwMFAuLi5atWqVRowYoVataoYSGRlZ65xbtmyxnvNSrgsAAC4fro68eEJCguLi4tS/f39FRkZq0aJFKigo0MSJEyXVPOL69ttvre9COnjwoLKyshQREaFjx45p3rx52rdvn958803rOR977DH99re/1QsvvKA77rhD//73v/XRRx9Zv/1Wl+sCAAA4NCSNGTNGR48e1Zw5c1RUVKSePXtq06ZNCg4OliQVFRXZvLuourpaL7/8sg4cOCA3NzfdcsstyszMVEhIiLVPVFSUVq1apSeffFIzZ87Utddeq9WrVysiIqLO1wUAAHDoe5KcGe9JAgDA+TjFe5IAAACaM0ISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAACACUISAABoFsrKpCNHzPcdOVKzvykRkgAAgMOVlUnDhkk33ywVFtruKyysaR82rGmDEiEJAAA4XEWFVFIiHT4sDRr0c1AqLKz5fPhwzf6KiqariZAEAAAcLjBQSk+XwsJ+DkqZmT8HpLCwmv2BgU1Xk2vTXQoAAODCgoJqgtC5YDRwYE37uYAUFNS09XAnCQAANBtBQdKKFbZtK1Y0fUCSCEkAAKAZKSyU4uJs2+Liai/mbgqEJAAA0Cycv0g7LEz67DPbNUpNHZQISQAAwOGOHKm9SDsqqvZi7gu9R6kxsHAbAAA4nLe31KFDzb/PX6R9/mLuDh1q+jUVh99JSk5OVmhoqDw9PRUeHq5t27ZdtP/bb7+tPn36qHXr1goICND999+vo0ePWvcPGjRIFoul1nb77bdb+8yaNavW/o4dOzbaGAEAwMX5+EibN0sZGbUXaQcF1bRv3lzTr6k4NCStXr1a8fHxmjFjhnJychQdHa3Y2FgVFBSY9t++fbvuu+8+PfDAA/ryyy/17rvv6vPPP9ef/vQna5+1a9eqqKjIuu3bt08uLi666667bM7Vo0cPm3579+5t1LECAICL8/G58HuQAgObNiBJDg5J8+bN0wMPPKA//elP6tatm+bPn6+goCClpKSY9t+5c6dCQkI0ZcoUhYaG6je/+Y0efvhhZWdnW/u0a9dOHTt2tG5paWlq3bp1rZDk6upq0699+/aNOlYAAOBcHBaSTp06pd27dysmJsamPSYmRpmZmabHREVF6ciRI9q0aZMMw9D333+v9957z+ZR2i8tXbpUY8eOVZs2bWzaDx06pE6dOik0NFRjx47V4cOHL31QAACgxXBYSCotLVV1dbX8/f1t2v39/VVcXGx6TFRUlN5++22NGTNG7u7u6tixo6688kq99tprpv2zsrK0b98+m8dxkhQREaHly5frww8/1OLFi1VcXKyoqCibtU2/VFVVpfLycpsNAAC0XA5fuG2xWGw+G4ZRq+2c/fv3a8qUKXrqqae0e/dubd68Wfn5+Zo4caJp/6VLl6pnz5666aabbNpjY2M1evRo9erVS0OGDNHGjRslSW+++eYF60xKSpKPj491C3LEqz8BAECTcVhI8vPzk4uLS627RiUlJbXuLp2TlJSkgQMH6q9//at69+6toUOHKjk5WampqSoqKrLpW1lZqVWrVtW6i2SmTZs26tWrlw4dOnTBPomJiSorK7NuhY549ScAAGgyDgtJ7u7uCg8PV1pamk17WlqaoqKiTI+prKxUq1a2Jbu4uEiquQN1vnfeeUdVVVW69957f7WWqqoq5eXlKSAg4IJ9PDw81LZtW5sNAAC0XA593JaQkKAlS5YoNTVVeXl5mjp1qgoKCqyPzxITE3XfffdZ+48cOVJr165VSkqKDh8+rM8++0xTpkzRTTfdpE6dOtmce+nSpRo1apSuuuqqWtedNm2aMjIylJ+fr127dunOO+9UeXm5xo8f37gDBgAATsOhb9weM2aMjh49qjlz5qioqEg9e/bUpk2bFBwcLEkqKiqyeWfShAkTVFFRoQULFugvf/mLrrzySt1666164YUXbM578OBBbd++XVu2bDG97pEjRzRu3DiVlpaqffv2GjBggHbu3Gm9LgAAgMX45XMq1El5ebl8fHxUVlbGozcAAJyEPX+/+e22ejqXLXkVAAAAzuPc3+263CMiJNVTRUWFJPEqAAAAnFBFRYV8fuV3TnjcVk9nz57Vd999J29v7wu+16m+ysvLFRQUpMLCwhb5KI/xOb+WPsaWPj6p5Y+R8Tm/xhqjYRiqqKhQp06dan1j/pe4k1RPrVq1UuCFfoWvgbT0Vw0wPufX0sfY0scntfwxMj7n1xhj/LU7SOc4/I3bAAAAzREhCQAAwAQhqRny8PDQ008/LQ8PD0eX0igYn/Nr6WNs6eOTWv4YGZ/zaw5jZOE2AACACe4kAQAAmCAkAQAAmCAkAQAAmCAkAQAAmCAkOUhycrJCQ0Pl6emp8PBwbdu27aL9MzIyFB4eLk9PT4WFhen1119vokrrx57xpaeny2Kx1Nq++uqrJqy47rZu3aqRI0eqU6dOslgsWr9+/a8e40zzZ+/4nG3+kpKSdOONN8rb21sdOnTQqFGjdODAgV89zlnmsD7jc7Y5TElJUe/eva0vGYyMjNQHH3xw0WOcZf4k+8fnbPP3S0lJSbJYLIqPj79oP0fMISHJAVavXq34+HjNmDFDOTk5io6OVmxsrAoKCkz75+fna/jw4YqOjlZOTo6eeOIJTZkyRWvWrGniyuvG3vGdc+DAARUVFVm3zp07N1HF9jl58qT69OmjBQsW1Km/s82fveM7x1nmLyMjQ5MmTdLOnTuVlpamM2fOKCYmRidPnrzgMc40h/UZ3znOMoeBgYH6+9//ruzsbGVnZ+vWW2/VHXfcoS+//NK0vzPNn2T/+M5xlvk73+eff65Fixapd+/eF+3nsDk00ORuuukmY+LEiTZt119/vTF9+nTT/n/729+M66+/3qbt4YcfNgYMGNBoNV4Ke8f36aefGpKMY8eONUF1DUuSsW7duov2cbb5O19dxufM82cYhlFSUmJIMjIyMi7Yx5nnsC7jc/Y5NAzD8PX1NZYsWWK6z5nn75yLjc9Z56+iosLo3LmzkZaWZtx8883GY489dsG+jppD7iQ1sVOnTmn37t2KiYmxaY+JiVFmZqbpMTt27KjVf+jQocrOztbp06cbrdb6qM/4zunbt68CAgI0ePBgffrpp41ZZpNypvm7FM46f2VlZZKkdu3aXbCPM89hXcZ3jjPOYXV1tVatWqWTJ08qMjLStI8zz19dxneOs83fpEmTdPvtt2vIkCG/2tdRc0hIamKlpaWqrq6Wv7+/Tbu/v7+Ki4tNjykuLjbtf+bMGZWWljZarfVRn/EFBARo0aJFWrNmjdauXauuXbtq8ODB2rp1a1OU3Oicaf7qw5nnzzAMJSQk6De/+Y169ux5wX7OOod1HZ8zzuHevXt1xRVXyMPDQxMnTtS6devUvXt3077OOH/2jM8Z52/VqlX673//q6SkpDr1d9QcujbamXFRFovF5rNhGLXafq2/WXtzYc/4unbtqq5du1o/R0ZGqrCwUC+99JJ++9vfNmqdTcXZ5s8ezjx/kydP1hdffKHt27f/al9nnMO6js8Z57Br167Kzc3V8ePHtWbNGo0fP14ZGRkXDBLONn/2jM/Z5q+wsFCPPfaYtmzZIk9Pzzof54g55E5SE/Pz85OLi0utuyolJSW1UvI5HTt2NO3v6uqqq666qtFqrY/6jM/MgAEDdOjQoYYuzyGcaf4aijPM36OPPqoNGzbo008/VWBg4EX7OuMc2jM+M819Dt3d3XXdddepf//+SkpKUp8+ffSPf/zDtK8zzp894zPTnOdv9+7dKikpUXh4uFxdXeXq6qqMjAy9+uqrcnV1VXV1da1jHDWHhKQm5u7urvDwcKWlpdm0p6WlKSoqyvSYyMjIWv23bNmi/v37y83NrdFqrY/6jM9MTk6OAgICGro8h3Cm+WsozXn+DMPQ5MmTtXbtWn3yyScKDQ391WOcaQ7rMz4zzXkOzRiGoaqqKtN9zjR/F3Kx8ZlpzvM3ePBg7d27V7m5udatf//+uueee5SbmysXF5daxzhsDht1WThMrVq1ynBzczOWLl1q7N+/34iPjzfatGljfP3114ZhGMb06dONuLg4a//Dhw8brVu3NqZOnWrs37/fWLp0qeHm5ma89957jhrCRdk7vldeecVYt26dcfDgQWPfvn3G9OnTDUnGmjVrHDWEi6qoqDBycnKMnJwcQ5Ixb948Iycnx/jmm28Mw3D++bN3fM42f4888ojh4+NjpKenG0VFRdatsrLS2seZ57A+43O2OUxMTDS2bt1q5OfnG1988YXxxBNPGK1atTK2bNliGIZzz59h2D8+Z5s/M7/8dltzmUNCkoMsXLjQCA4ONtzd3Y1+/frZfD13/Pjxxs0332zTPz093ejbt6/h7u5uhISEGCkpKU1csX3sGd8LL7xgXHvttYanp6fh6+tr/OY3vzE2btzogKrr5tzXbX+5jR8/3jAM558/e8fnbPNnNjZJxhtvvGHt48xzWJ/xOdsc/vGPf7T+70v79u2NwYMHWwOEYTj3/BmG/eNztvkz88uQ1Fzm0GIY///KJwAAAFixJgkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkALoHFYtH69esdXQaARkBIAuC0JkyYIIvFUmsbNmyYo0sD0AK4OroAALgUw4YN0xtvvGHT5uHh4aBqALQk3EkC4NQ8PDzUsWNHm83X11dSzaOwlJQUxcbGysvLS6GhoXr33Xdtjt+7d69uvfVWeXl56aqrrtJDDz2kEydO2PRJTU1Vjx495OHhoYCAAE2ePNlmf2lpqX7/+9+rdevW6ty5szZs2GDdd+zYMd1zzz1q3769vLy81Llz51qhDkDzREgC0KLNnDlTo0eP1p49e3Tvvfdq3LhxysvLkyRVVlZq2LBh8vX11eeff653331XH330kU0ISklJ0aRJk/TQQw9p79692rBhg6677jqba8yePVt33323vvjiCw0fPlz33HOP/t//+3/W6+/fv18ffPCB8vLylJKSIj8/v6b7DwCg/hr9J3QBoJGMHz/ecHFxMdq0aWOzzZkzxzAMw5BkTJw40eaYiIgI45FHHjEMwzAWLVpk+Pr6GidOnLDu37hxo9GqVSujuLjYMAzD6NSpkzFjxowL1iDJePLJJ62fT5w4YVgsFuODDz4wDMMwRo4cadx///0NM2AATYo1SQCc2i233KKUlBSbtnbt2ln/HRkZabMvMjJSubm5kqS8vDz16dNHbdq0se4fOHCgzp49qwMHDshisei7777T4MGDL1pD7969rf9u06aNvL29VVJSIkl65JFHNHr0aP33v/9VTEyMRo0apaioqHqNFUDTIiQBcGpt2rSp9fjr11gsFkmSYRjWf5v18fLyqtP53Nzcah179uxZSVJsbKy++eYbbdy4UR999JEGDx6sSZMm6aWXXrKrZgBNjzVJAFq0nTt31vp8/fXXS5K6d++u3NxcnTx50rr/s88+U6tWrdSlSxd5e3srJCREH3/88SXV0L59e02YMEFvvfWW5s+fr0WLFl3S+QA0De4kAXBqVVVVKi4utmlzdXW1Lo5+99131b9/f/3mN7/R22+/raysLC1dulSSdM899+jpp5/W+PHjNWvWLP3www969NFHFRcXJ39/f0nSrFmzNHHiRHXo0EGxsbGqqKjQZ599pkcffbRO9T311FMKDw9Xjx49VFVVpf/85z/q1q1bA/4XANBYCEkAnNrmzZsVEBBg09a1a1d99dVXkmq+ebZq1Sr9+c9/VseOHfX222+re/fukqTWrVvrww8/1GOPPaYbb7xRrVu31ujRozVv3jzrucaPH6+ffvpJr7zyiqZNmyY/Pz/deeedda7P3d1diYmJ+vrrr+Xl5aXo6GitWrWqAUYOoLFZDMMwHF0EADQGi8WidevWadSoUY4uBYATYk0SAACACUISAACACdYkAWixWE0A4FJwJwkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMAEIQkAAMDE/wefC+bKSI55mwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(ce_train_loss)),\n",
    "            ce_train_loss, c='b', marker='x')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sbert, 'sbert_mini.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_saved = torch.load('sbert_mini.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4369, -1.1700,  1.3116], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to apply the model to a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sbert(model, token_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model.embeddings(token_indices)\n",
    "        v = model.transformer_encoder(v).mean(dim=0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8014,  0.6560,  0.7559,  0.0487, -0.5466, -0.1760, -0.3141,  0.3348,\n",
       "        -0.1782, -0.4207, -0.2321,  1.5978,  0.0203,  0.0886, -0.0198, -1.2385,\n",
       "         1.0829,  0.8787, -1.1464, -1.8211,  0.8561,  0.0498, -0.0609, -0.7358,\n",
       "        -0.2515, -1.7329,  0.8171, -0.3011, -0.5695,  1.6172,  0.5847,  1.2974,\n",
       "        -0.0680,  1.0012, -0.9143, -1.4530, -0.5814,  0.2806,  0.3949,  1.1775,\n",
       "         0.5916, -0.0531, -0.5872, -0.9889, -0.3754,  0.3764,  0.6734, -0.0198,\n",
       "        -0.0795,  0.5077])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sbert(sbert, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'contradiction', 2: 'neutral'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to all our pairs and, for each pair, we compute the cosine of the resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:06<00:00, 832.31it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "sbert.eval()\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        encode_sbert(sbert, data[0]),\n",
    "        encode_sbert(sbert, data[1]))\n",
    "    cos_sim[idx2label[data[2].item()]] += cos_val\n",
    "    cnt[idx2label[data[2].item()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entailment': tensor(1254.2577),\n",
       "  'neutral': tensor(1004.2047),\n",
       "  'contradiction': tensor(775.5323)},\n",
       " {'entailment': 1839.0, 'neutral': 1821.0, 'contradiction': 1833.0})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.6820),\n",
       " 'neutral': tensor(0.5515),\n",
       " 'contradiction': tensor(0.4231)}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Embedder to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'is', 'lovely', 'today', '.'],\n",
       " ['it', \"'\", 's', 'so', 'sunny', 'outside', '!'],\n",
       " ['he', 'drove', 'to', 'the', 'stadium', '.']]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = [tokenize(sent, pattern) for sent in sentences]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    3,  1623,    17, 11130,   376,     5]),\n",
       " tensor([  23,   60, 1537,  103, 9328,  590,  808]),\n",
       " tensor([  21, 3189,    7,    3, 1355,    5])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents = [convert_symbols(sent, token2idx) for sent in tokenized_sents]\n",
    "indexed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-3.6002e-01,  9.7860e-01, -4.6371e-01,  2.1167e-01, -4.1522e-02,\n",
       "         -5.9220e-01, -7.6979e-01, -4.2192e-01,  2.0117e-01, -1.2148e+00,\n",
       "          5.3915e-01,  7.2086e-01,  5.2815e-01,  3.4336e-01, -3.9807e-02,\n",
       "         -1.5481e+00, -1.2093e+00,  2.5962e-02, -6.1465e-01, -5.6964e-01,\n",
       "         -5.4577e-01,  2.9394e-01, -2.1570e-01,  9.5261e-01,  2.7019e-01,\n",
       "         -6.7463e-01,  1.3972e+00, -1.9279e-01,  3.5067e-01,  1.4340e+00,\n",
       "          1.1645e-01,  1.6451e+00,  1.2817e-01,  1.1872e+00, -1.9973e+00,\n",
       "         -1.0094e+00, -9.4188e-01,  4.8529e-01, -2.1123e-01,  2.8514e+00,\n",
       "         -6.3487e-01, -4.3191e-01, -2.1509e-01, -7.4816e-01, -5.9541e-01,\n",
       "         -9.1774e-04,  1.0423e+00,  1.7864e-01, -5.2498e-01,  9.2983e-01]),\n",
       " tensor([-0.6334,  0.3495,  0.1583, -0.1137,  0.2745, -0.8847, -0.1210, -0.5381,\n",
       "          0.3633, -0.3666,  0.2742,  1.4850, -0.0218,  0.5313,  0.5570, -1.1836,\n",
       "         -1.3056, -0.0524, -0.4834, -1.1695, -0.9242,  0.2665,  0.0371,  0.7770,\n",
       "         -0.0965, -0.4902,  0.6588, -0.1064,  0.2758,  0.7374,  0.1331,  1.0800,\n",
       "         -0.0631,  1.1681, -1.1333, -0.1264, -0.6553,  0.5880, -0.2212,  1.7230,\n",
       "          0.1271, -0.6599,  0.6866, -1.6651, -0.4398, -0.2536,  0.5341,  0.2611,\n",
       "          0.1347,  0.5478]),\n",
       " tensor([ 0.1200,  0.0756,  1.7675, -0.5079, -0.7582, -0.3370,  0.4255,  0.6508,\n",
       "          0.1597, -1.1875, -0.6057,  1.0690, -0.6125,  0.2140,  0.2164, -0.4505,\n",
       "          1.3087,  0.8294, -1.6822, -1.8268,  1.2078,  0.1759, -1.1053, -2.0403,\n",
       "         -1.1756, -1.3670,  1.6508, -1.5301, -0.1476,  0.9084,  0.7028,  1.7953,\n",
       "         -0.0399,  1.2869,  0.2712, -0.7260, -0.3599, -0.1822,  0.7863,  0.4772,\n",
       "          0.5026, -0.6484, -0.3003, -0.8569, -0.2574, -0.4122,  0.1782,  1.1470,\n",
       "         -0.0700,  1.3016])]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sents = [encode_sbert(sbert, sent) for sent in indexed_sents]\n",
    "encoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8257, 0.3007],\n",
       "        [0.8257, 1.0000, 0.3301],\n",
       "        [0.3007, 0.3301, 1.0000]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([compute_cosine(s1, s2)\n",
    "              for s1 in encoded_sents\n",
    "              for s2 in encoded_sents]).reshape(len(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Write a short individual report on your program. I recommend that you use this structure for your report:\n",
    "      1. Objectives and dataset\n",
    "      2. Method and program structure, where you should outline your program and possibly describe difficult parts.\n",
    "      3. Results.\n",
    "      4. Conclusion.\n",
    "2. In Sect. _Method and program structure_, do not forget to:\n",
    "   * Summarize the baseline\n",
    "   * Summarize SBERT\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, use Latex. This will probably help you structure your text. You can use the Overleaf online editor (www.overleaf.com). You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 18, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
