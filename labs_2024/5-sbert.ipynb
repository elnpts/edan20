{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: A sentence embedder\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a sentence embedder simplified from Reimers and Gurevych's Sentence-BERT: https://arxiv.org/pdf/1908.10084. S-BERT is written in PyTorch and its code is available from GitHub: https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "The objectives of the assignment are to:\n",
    "* Write a program to embed sentences\n",
    "* Use neural networks with PyTorch\n",
    "* Write a short report of 2 to 3 pages to describe your program.\n",
    "\n",
    "Note: Should your machine be unable to train a model for the whole dataset, then use only a fraction of the dataset such as 10% or less. For this, use the `MINI_CORPUS` constant. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We saw we can vectorize words using a dense representation. We can extend this to documents. This enables us to store the resulting vectors in databases and then use fast algorithms for paragraph or document comparisons such as Faiss: https://github.com/facebookresearch/faiss\n",
    "\n",
    "There are many document vectorization techniques and models are regularly benchmarked, see: https://huggingface.co/spaces/mteb/leaderboard. See also a list of available vector databases here https://db-engines.com/en/ranking/vector+dbms\n",
    "\n",
    "In this lab, you will program two techniques to vectorize documents into dense vectors. You will first implement a baseline technique and then a toy version of SBERT. SBERT is one of the earliest transformer-based document vectorization algorithm: _Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks_ by Reimers and Gurevych (2019) https://arxiv.org/pdf/1908.10084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the algorithms\n",
    "Read the Getting Started paragraph of https://github.com/UKPLab/sentence-transformers for an overview.\n",
    "\n",
    "Read the summary of the SBERT paper as well as Sections 1 and 3, _Introduction_ and _Model_. In the triplet objective function, an anchor is a start sample, the positive sample is close to the anchor, while the negative one is different. Considering a language detector, think of a sentence in Swedish as the anchor. A positive sample would be another sentence in Swedish and a negative one could be a sentence in English.\n",
    "\n",
    "In the _Method and program struture_ section of your report, you will summarize these sections in 10 to 15 lines. Note that a three-way softmax classifier is simply a logistic regression with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import regex as re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a reduced dataset for the development of your program with `MINI_CORPUS` set to `True`. Once your program is ready, you can train your model on the whole dataset (if you have the time). Set `MINI_CORPUS` to `False` then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_CORPUS = True  # Set the value to True when you develop the program\n",
    "MINI_PERCENTAGE = 0.01  # Percentage of the original dataset.\n",
    "# Depending on your machine, you may even use less than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: SNLI\n",
    "As dataset, you will use SNLI. SNLI consists of over 500,000 lines with the text of the pairs and their labels. The authors created the dataset by giving volunteers a sentence (the premise) and asking them to write a second sentence (the hypothesis) that is either definitely true\n",
    "(entailment), that might be true (neutral), or that is definitely false (contradiction).\n",
    "\n",
    "Read the dataset description from this URL https://nlp.stanford.edu/projects/snli/ and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adjust the path to fit your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('corpus/snli_1.0/snli_1.0_train.jsonl', 'r') as f:\n",
    "    dataset_list = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = []\n",
    "for json_str in dataset_list:\n",
    "    dataset_snli += [json.loads(json_str)]\n",
    "    # print(f\"result: {result}\")\n",
    "    # print(isinstance(result, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with an agreement in the annotation. The final annotation is the gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['entailment'],\n",
       " 'captionID': '3706019259.jpg#3',\n",
       " 'gold_label': 'entailment',\n",
       " 'pairID': '3706019259.jpg#3r2e',\n",
       " 'sentence1': 'A foreign family is walking along a dirt path next to the water.',\n",
       " 'sentence1_binary_parse': '( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))',\n",
       " 'sentence2': 'A family of foreigners walks by the water.',\n",
       " 'sentence2_binary_parse': '( ( ( A family ) ( of foreigners ) ) ( ( walks ( by ( the water ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN of) (NP (NNS foreigners)))) (VP (VBZ walks) (PP (IN by) (NP (DT the) (NN water)))) (. .)))'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with no agreement in the annotation. The gold label is `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['contradiction', 'contradiction', 'neutral', 'neutral'],\n",
       " 'captionID': '2677109430.jpg#2',\n",
       " 'gold_label': '-',\n",
       " 'pairID': '2677109430.jpg#2r1c',\n",
       " 'sentence1': 'A small group of church-goers watch a choir practice.',\n",
       " 'sentence1_binary_parse': '( ( ( A ( small group ) ) ( of church-goers ) ) ( ( watch ( a ( choir practice ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (JJ small) (NN group)) (PP (IN of) (NP (NNS church-goers)))) (VP (VBP watch) (NP (DT a) (NN choir) (NN practice))) (. .)))',\n",
       " 'sentence2': 'A choir performs in front of packed crowd.',\n",
       " 'sentence2_binary_parse': '( ( A choir ) ( ( performs ( in ( front ( of ( packed crowd ) ) ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (DT A) (NN choir)) (VP (VBZ performs) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ packed) (NN crowd)))))) (. .)))'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the samples that have no agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = []\n",
    "for sample in dataset_snli:\n",
    "    s1 = sample['sentence1']\n",
    "    s2 = sample['sentence2']\n",
    "    label = sample['gold_label']\n",
    "    if label != '-':\n",
    "        dataset_str += [(s1, s2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str) # 549367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is at a diner, ordering an omelette.',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is outdoors, on a horse.',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_CORPUS:\n",
    "    new_size = int(len(dataset_str) * MINI_PERCENTAGE)\n",
    "    dataset_str = dataset_str[:new_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str) # 5493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: GloVe\n",
    "\n",
    "You will first implement a baseline, an easy technique that serves as comparison for more elaborate ones. \n",
    "\n",
    "In Sect. 4.1 and Table 1 the authors proposed a baseline technique for computing a semantic\n",
    "textual similarity between two sentences that uses GloVe embeddings. Describe this technique in 5 to 10 lines in the _Method and program struture_ section. You will create a *Baseline* subsection for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "You will use a list of pretrained word embeddings to implement the baseline and GloVe is one such vector lists. GloVe is available in different dimensionalities (50, 100, 200, 300) and vocabulary sizes (400,000 words, 1.2M, 2.4M). \n",
    "\n",
    "Download the GloVe 6B embeddings from https://nlp.stanford.edu/projects/glove/, uncompress it, and keep the `glove.6B.50d.txt` file of 400,000 words with 50-dimensional vectors.\n",
    "\n",
    "Please adjust your path to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'corpus/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor(\n",
    "            list(map(float, values[1:])), dtype=torch.float32)\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = next(iter(embeddings.values())).size()[0]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the words in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "glove = []\n",
    "for word, vector in embeddings.items():\n",
    "    glove_words += [word]\n",
    "    glove += [vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[:10] # ['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a tensor with the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.stack(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of word embeddings (400,000) and their dimension (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size() # [400000, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = glove.size()[1]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserve three special symbols: padding, unknown, and the first classification token of BERT. See the lecture on transformers for a clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['[PAD]', '[UNK]', '[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', 'the', ',', '.', 'of', 'to', 'and', 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = specials + glove_words\n",
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the vectors for the special tokens to our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.vstack((torch.zeros((3, d_model)), glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400003, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenization\n",
    "You will now tokenize the sentences\n",
    "\n",
    "Write a regular expression that tokenizes the words, numbers, and punctuation. Use Unicode classes. Note that a punctuation is a single symbol while the words and numbers are sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "pattern = r'\\p{L}+|\\p{N}+|\\p{P}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tokenization function that takes a string as input and results a list of tokens. Set the string in lower case by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize(sentence, pattern, lc=True):\n",
    "    if lc:\n",
    "        sentence = sentence.lower()\n",
    "    \n",
    "    return re.findall(pattern, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(dataset_str[0][0], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code that, for each sample of your dataset, builds a triple consisting of:\n",
    "1. The first tokenized sentence, \n",
    "2. The second one, and \n",
    "3. The class\n",
    "\n",
    "Build a list of all these triples to represent your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset_tokens = []\n",
    "for sample in dataset_str:\n",
    "    s1 = tokenize(sample[0], pattern)\n",
    "    s2 = tokenize(sample[1], pattern)\n",
    "    l = sample[2]\n",
    "    dataset_tokens.append((s1, s2, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5493"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokens) # 5493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build token-to-index `token2idx` and index-to-token `idx2token` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {}\n",
    "idx2token = {}\n",
    "idx = 0\n",
    "for word in glove_words:\n",
    "    token2idx[word] =  idx\n",
    "    idx2token[idx] = word\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['the'], token2idx['a'] # (3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token[3], idx2token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the set of all the labels (classes) from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "labels = []\n",
    "for i in range(len(dataset_tokens)):\n",
    "    labels.append(dataset_tokens[i][2])\n",
    "\n",
    "labels = list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'entailment', 'neutral']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build label-to-index `label2idx` and index-to-label `idx2label` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "idx2label = {idx: label for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contradiction': 0, 'entailment': 1, 'neutral': 2}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'contradiction', 1: 'entailment', 2: 'neutral'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to convert:\n",
    "  * A list of tokens into a list of `LongTensor` indices and \n",
    "  * The class to a tensor. \n",
    "\n",
    "Your function should be able to handle two types: either a list or a string. The tokens are strored in a list and the class (label) is a string\n",
    "\n",
    "Note that an unknown token in GloVe should be mapped to the `UNK` symbol of index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def convert_symbols(symbols, symbol2idx):\n",
    "    if type(symbols) is str:\n",
    "        if symbols in symbol2idx:\n",
    "            return torch.LongTensor([symbol2idx[symbols]])\n",
    "        else:\n",
    "            return torch.LongTensor([1])\n",
    "    elif type(symbols) is list:\n",
    "        return torch.LongTensor([symbol2idx[token] if token in symbol2idx else 1 for token in symbols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a list of tokens into a `LongTensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols('neutrall', label2idx) # tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a label string into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][2], label2idx) # tensor(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown tokens have the index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'person', 'on', 'a', 'horsewww']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('a person on a horsewww', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 902,  16,  10,   1,   1])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(tokenize('a person on a horsewww wxwx', pattern), token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tokens and labels in your dataset by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for sample in dataset_tokens:\n",
    "    s1 = sample[0] \n",
    "    s2 = sample[1]  \n",
    "    label = sample[2]  \n",
    "    \n",
    "    convert1 = convert_symbols(s1, token2idx)\n",
    "    convert2 = convert_symbols(s2, token2idx)\n",
    "    convert3 = convert_symbols(label, label2idx)\n",
    "    \n",
    "    tokenized_sample = (convert1, convert2, convert3)\n",
    "    \n",
    "    dataset.append(tokenized_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([    10,    902,     17,     25,     10,  19304,      4,   7490,     32,\n",
       "         119031,      5]),\n",
       " tensor([0]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Embedding` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the GloVe vectors in a PyTorch `Embedding` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embs = nn.Embedding(glove.size()[0],\n",
    "                          glove.size()[1],\n",
    "                          padding_idx=0).from_pretrained(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the embedding for _the_ with its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embs(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Mean of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_str = dataset_str[0][0]\n",
    "s2_str = dataset_str[0][1]\n",
    "s3_str = dataset_str[0][2]\n",
    "s1_str, s2_str, s3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_idx = dataset[0][0]\n",
    "s2_idx = dataset[0][1]\n",
    "s3_idx = dataset[0][2]\n",
    "s1_idx, s2_idx, s3_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of indices and the GloVe embeddings as input and that computes the mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def mean_embs(input_idx: torch.LongTensor, glove_embs: nn.Embedding) -> torch.tensor:\n",
    "    embeddings = glove_embs(input_idx) # embeddings for input idx\n",
    "    mean_embedding = embeddings.mean(dim=0) # Compute the mean of the embeddings along the first dimension (rows)\n",
    "    return mean_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "         0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "        -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "         0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "        -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "        -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "         0.2139, -0.1993])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embs(dataset[0][0], glove_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the cosine of two vectors. You will return `torch.tensor(0.0)` if one of the vectors is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_cosine(v1: torch.tensor, v2: torch.tensor) -> torch.tensor:\n",
    "    dot_product = torch.dot(v1, v2)\n",
    "    norm1 = torch.norm(v1)\n",
    "    norm2 = torch.norm(v2)\n",
    "    return dot_product / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "          0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "         -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "          0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "         -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "         -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "          0.2139, -0.1993]),\n",
       " tensor([ 1.0878e-01,  3.7231e-01, -4.7114e-01, -1.3591e-02,  5.1340e-01,\n",
       "          3.7193e-01, -4.4592e-01, -5.9900e-02,  2.5352e-01, -1.3076e-01,\n",
       "          1.6231e-01,  2.3146e-03, -3.6474e-01,  2.3840e-03,  3.4653e-01,\n",
       "         -2.1769e-01,  2.5946e-02,  3.9537e-01, -6.9517e-01, -3.4811e-01,\n",
       "         -4.9454e-02,  1.4977e-01, -1.2447e-01,  8.1851e-02,  6.2581e-02,\n",
       "         -1.8692e+00, -3.1502e-01, -7.4079e-02,  1.2478e-01, -1.6717e-02,\n",
       "          3.3707e+00,  8.7725e-02, -4.0180e-01, -1.8131e-01,  2.5315e-01,\n",
       "          1.7589e-01,  2.2877e-01,  4.3286e-01, -1.6315e-02, -3.6988e-01,\n",
       "         -2.8208e-02,  3.1538e-02, -2.4721e-01,  2.5963e-01,  1.3792e-02,\n",
       "         -2.6431e-01,  1.7081e-02, -2.0042e-01,  1.6257e-01,  2.1471e-01]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = mean_embs(s1_idx, glove_embs)\n",
    "v2 = mean_embs(s2_idx, glove_embs)\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9426)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the cosine of pairs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:00<00:00, 7066.64it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        mean_embs(data[0], glove_embs),\n",
    "        mean_embs(data[1], glove_embs))\n",
    "    class_name = idx2label[data[2].item()]\n",
    "    cos_sim[class_name] += cos_val\n",
    "    cnt[class_name] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will comment these values in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.9453),\n",
       " 'neutral': tensor(0.9385),\n",
       " 'contradiction': tensor(0.9298)}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT: The Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the SBERT architecture and replicate the pipeline in Fig. 1 in the paper. In the next cells, we walk through the figure from the bottom to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer builds an input consisting of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,  360, 5453]), tensor([   3,  194,  368, 2929]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the small cat', pattern))))\n",
    "p2 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the very big dog', pattern))))\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the BERT layer. Using PyTorch classes, create an encoder of four layers where each layer has five heads. You will use the classes `TransformerEncoderLayer` and `TransformerEncoder`. The dimensionality `d_model` is 50 as this is the size of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=50, nhead=5)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "      (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a BERT encoder. We associate each input index to an embedding. In the next cell, we create three embedding vectors correponding to three words.\n",
    "\n",
    "In the rest of the program, all our batches will have only one sample to eliminate the need for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6933, 0.1429, 0.9140, 0.4893, 0.6887, 0.9853, 0.2801, 0.3912, 0.7853,\n",
       "         0.8362, 0.5402, 0.2338, 0.9534, 0.7624, 0.9155, 0.6521, 0.9500, 0.4629,\n",
       "         0.6964, 0.7109, 0.4113, 0.5090, 0.7841, 0.5727, 0.1560, 0.7347, 0.2250,\n",
       "         0.1458, 0.4249, 0.4764, 0.9364, 0.2920, 0.7261, 0.0839, 0.5706, 0.0435,\n",
       "         0.9961, 0.0230, 0.6003, 0.2666, 0.8228, 0.4535, 0.3267, 0.9073, 0.4912,\n",
       "         0.3722, 0.0672, 0.7209, 0.5975, 0.5246],\n",
       "        [0.8129, 0.0616, 0.6723, 0.2152, 0.4996, 0.6258, 0.2393, 0.0910, 0.0167,\n",
       "         0.3030, 0.9990, 0.5135, 0.0263, 0.8944, 0.7529, 0.5402, 0.2570, 0.2289,\n",
       "         0.6952, 0.1081, 0.9593, 0.8603, 0.1289, 0.2404, 0.9614, 0.3570, 0.4361,\n",
       "         0.7982, 0.7335, 0.3081, 0.2074, 0.0857, 0.3527, 0.6576, 0.6455, 0.9586,\n",
       "         0.7347, 0.1371, 0.1741, 0.5382, 0.9597, 0.0618, 0.4509, 0.5499, 0.6692,\n",
       "         0.5892, 0.4026, 0.4598, 0.3038, 0.3354],\n",
       "        [0.6413, 0.2621, 0.9315, 0.1461, 0.6683, 0.4227, 0.7348, 0.2067, 0.6423,\n",
       "         0.9080, 0.3908, 0.5236, 0.0781, 0.8424, 0.1359, 0.7605, 0.9364, 0.0786,\n",
       "         0.0597, 0.7337, 0.5718, 0.2534, 0.3631, 0.6573, 0.6111, 0.6741, 0.8147,\n",
       "         0.1757, 0.8161, 0.3463, 0.1233, 0.9577, 0.3864, 0.1977, 0.8561, 0.5690,\n",
       "         0.2124, 0.3416, 0.8182, 0.4183, 0.0238, 0.0041, 0.6071, 0.4498, 0.0277,\n",
       "         0.7156, 0.4801, 0.1538, 0.2964, 0.4128]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(3, d_model)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it to the encoder to encode the input into three vectors of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.4936e-02, -9.4775e-01,  1.3116e-01,  1.3773e-02,  1.9177e-01,\n",
       "         -1.1852e+00,  9.1953e-01,  4.8722e-01, -2.0090e+00, -1.3034e+00,\n",
       "          9.8612e-01, -6.6265e-01,  1.6080e+00,  1.7855e+00,  1.0401e+00,\n",
       "          1.6420e+00,  7.3636e-01, -4.0315e-01,  1.7628e+00,  1.0439e+00,\n",
       "         -1.6682e+00,  1.3173e-01, -3.9403e-01,  1.5174e+00,  1.1624e+00,\n",
       "         -7.3881e-01,  1.8157e-01, -8.6243e-01, -1.2805e+00,  6.8261e-01,\n",
       "         -1.2390e+00,  2.6896e-01,  6.8355e-01, -1.1509e+00, -8.8052e-01,\n",
       "         -1.9866e-01, -5.1463e-01, -1.7994e+00, -5.0443e-01, -1.8488e-01,\n",
       "          3.2760e-01, -2.8020e-01,  1.7570e-01,  8.6453e-01,  1.1430e+00,\n",
       "         -1.1394e+00, -2.1244e-01,  3.1182e-01,  1.1748e+00, -1.3495e+00],\n",
       "        [ 7.9941e-01, -7.0400e-01, -1.7378e-01, -6.5652e-02,  4.0235e-01,\n",
       "         -1.4254e+00, -3.2587e-01, -4.6514e-02, -2.2728e+00, -6.1331e-01,\n",
       "          1.5886e+00, -6.7114e-01, -5.1413e-01,  1.1406e+00,  5.6544e-01,\n",
       "          9.0109e-01,  6.7824e-01, -4.3239e-01,  1.6182e+00,  9.1164e-01,\n",
       "         -4.9129e-01, -1.6303e-01, -1.4038e+00,  2.2681e+00,  2.6782e+00,\n",
       "         -9.5065e-01,  8.2034e-03, -2.3619e-01, -3.7185e-01,  3.2335e-02,\n",
       "         -2.1998e+00, -2.2044e-01,  4.4636e-01, -2.5493e-01, -1.0721e+00,\n",
       "          4.0228e-01,  1.0769e+00, -6.9369e-01, -3.2164e-01,  1.1764e+00,\n",
       "          6.1974e-02, -1.3151e+00,  3.6302e-02,  1.9832e-01,  7.8611e-01,\n",
       "         -9.8124e-01,  4.7197e-01,  2.8543e-01,  7.2410e-01, -1.3378e+00],\n",
       "        [-8.0140e-02, -3.2198e-01,  6.7699e-01, -3.8901e-01,  1.5837e-01,\n",
       "         -1.8342e+00,  7.3616e-01, -2.3649e-02, -1.3590e+00, -4.8927e-01,\n",
       "          1.4582e+00, -8.6794e-01,  4.5107e-01,  3.6603e-01,  6.0959e-01,\n",
       "          1.1194e+00,  1.4439e+00, -1.1876e-01,  1.3343e+00,  1.6537e+00,\n",
       "          4.6126e-01, -2.6790e-01, -1.1610e+00,  2.3352e+00,  1.6841e+00,\n",
       "         -6.4021e-01,  5.2887e-01, -8.0232e-01, -1.1956e+00,  4.4024e-01,\n",
       "         -2.2029e+00,  1.0776e+00, -1.2266e+00, -9.3375e-01, -2.5179e-01,\n",
       "          2.7638e-01, -4.3595e-01, -6.6242e-01,  6.2960e-02,  1.2631e+00,\n",
       "         -4.6425e-01, -8.4862e-01,  6.9694e-01,  8.8535e-02, -3.3463e-01,\n",
       "         -6.9120e-01,  1.8596e-01, -5.8043e-04,  8.2568e-01, -2.3309e+00]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a statement that will compute the mean of these embeddings. You will use the `mean(dim)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2181, -0.6579,  0.2115, -0.1470,  0.2508, -1.4816,  0.4433,  0.1390,\n",
       "        -1.8803, -0.8020,  1.3443, -0.7339,  0.5150,  1.0974,  0.7384,  1.2208,\n",
       "         0.9529, -0.3181,  1.5718,  1.2031, -0.5661, -0.0997, -0.9863,  2.0403,\n",
       "         1.8416, -0.7766,  0.2395, -0.6336, -0.9493,  0.3851, -1.8805,  0.3754,\n",
       "        -0.0322, -0.7799, -0.7348,  0.1600,  0.0421, -1.0518, -0.2544,  0.7515,\n",
       "        -0.0249, -0.8146,  0.3030,  0.3838,  0.5315, -0.9373,  0.1485,  0.1989,\n",
       "         0.9082, -1.6727], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "out.mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have understood the first steps of SBERT, we can implement them in a class.\n",
    "\n",
    "In the next cell, write the `forward()` method that takes the two sentences as input in the form of two `LongTensor` of indices.\n",
    "1. Extract their embeddings from the GloVe embedding matrix.\n",
    "2. Encode them with the transformer, and \n",
    "3. Compute their respective mean. You will call these vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "4. Return these two vectors of means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(\n",
    "            glove.size()[0],\n",
    "            glove.size()[1],\n",
    "            padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        # We do not use this last line for now\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        embedded_u = self.embeddings(u)\n",
    "        embedded_v = self.embeddings(v)\n",
    "        \n",
    "        encoded_u = self.transformer_encoder(embedded_u)\n",
    "        encoded_v = self.transformer_encoder(embedded_v)\n",
    "\n",
    "        u = encoded_u.mean(dim=0)\n",
    "        v = encoded_v.mean(dim=0)\n",
    "        \n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.5845e-01, -5.2890e-01, -4.8762e-01, -1.3063e+00, -1.0034e+00,\n",
       "         -1.1871e+00, -1.9258e+00,  3.1402e-01,  3.7742e-01,  8.5606e-02,\n",
       "          1.8564e-01,  5.3338e-01, -9.5420e-01,  7.4132e-02, -7.9633e-01,\n",
       "          1.0983e-01,  1.0855e-01,  3.6793e-01,  5.7258e-01, -1.5305e+00,\n",
       "         -1.5694e-01, -4.5694e-01,  1.7670e+00, -4.5193e-01,  5.1654e-01,\n",
       "         -1.2899e+00,  2.0670e+00,  1.6563e+00, -2.3063e-01, -1.7191e-01,\n",
       "          2.1753e+00, -2.1288e-03, -1.4163e-01,  9.8150e-01, -8.9934e-01,\n",
       "          6.9570e-01,  2.1982e+00,  4.8190e-01, -7.5968e-02,  9.2481e-01,\n",
       "         -6.4994e-01,  5.9280e-01, -1.3660e-01,  1.2234e-01, -1.1306e+00,\n",
       "          6.7706e-01, -1.0082e+00, -1.3872e+00, -7.6229e-02,  4.2236e-02],\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([ 0.5603, -0.4961, -0.5141, -1.1558, -0.5854, -1.1731, -2.0034,  0.4669,\n",
       "          0.5995, -0.0263,  0.3601,  0.4464, -0.9653,  0.0592, -0.9062,  0.1604,\n",
       "          0.1545,  0.2883,  0.5830, -1.3120, -0.3589, -0.5636,  1.4987, -0.3407,\n",
       "          0.1778, -1.2835,  2.2507,  1.6136, -0.4549, -0.1072,  2.3044,  0.1631,\n",
       "         -0.4090,  0.8577, -0.6559,  0.9349,  2.3509,  0.6579, -0.2288,  0.8931,\n",
       "         -0.9793,  0.3392, -0.3840,  0.0646, -1.2129,  0.3859, -1.1250, -1.2543,\n",
       "         -0.0730,  0.3979], grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that concatenate `u`, `v`, and `|u-v|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.5845e-01, -5.2890e-01, -4.8762e-01, -1.3063e+00, -1.0034e+00,\n",
       "        -1.1871e+00, -1.9258e+00,  3.1402e-01,  3.7742e-01,  8.5606e-02,\n",
       "         1.8564e-01,  5.3338e-01, -9.5420e-01,  7.4132e-02, -7.9633e-01,\n",
       "         1.0983e-01,  1.0855e-01,  3.6793e-01,  5.7258e-01, -1.5305e+00,\n",
       "        -1.5694e-01, -4.5694e-01,  1.7670e+00, -4.5193e-01,  5.1654e-01,\n",
       "        -1.2899e+00,  2.0670e+00,  1.6563e+00, -2.3063e-01, -1.7191e-01,\n",
       "         2.1753e+00, -2.1288e-03, -1.4163e-01,  9.8150e-01, -8.9934e-01,\n",
       "         6.9570e-01,  2.1982e+00,  4.8190e-01, -7.5968e-02,  9.2481e-01,\n",
       "        -6.4994e-01,  5.9280e-01, -1.3660e-01,  1.2234e-01, -1.1306e+00,\n",
       "         6.7706e-01, -1.0082e+00, -1.3872e+00, -7.6229e-02,  4.2236e-02,\n",
       "         5.6029e-01, -4.9606e-01, -5.1408e-01, -1.1558e+00, -5.8543e-01,\n",
       "        -1.1731e+00, -2.0034e+00,  4.6690e-01,  5.9948e-01, -2.6262e-02,\n",
       "         3.6009e-01,  4.4636e-01, -9.6532e-01,  5.9185e-02, -9.0620e-01,\n",
       "         1.6035e-01,  1.5454e-01,  2.8835e-01,  5.8295e-01, -1.3120e+00,\n",
       "        -3.5893e-01, -5.6361e-01,  1.4987e+00, -3.4069e-01,  1.7779e-01,\n",
       "        -1.2835e+00,  2.2507e+00,  1.6136e+00, -4.5488e-01, -1.0725e-01,\n",
       "         2.3044e+00,  1.6307e-01, -4.0895e-01,  8.5766e-01, -6.5595e-01,\n",
       "         9.3488e-01,  2.3509e+00,  6.5789e-01, -2.2884e-01,  8.9307e-01,\n",
       "        -9.7930e-01,  3.3917e-01, -3.8397e-01,  6.4638e-02, -1.2129e+00,\n",
       "         3.8594e-01, -1.1250e+00, -1.2543e+00, -7.2994e-02,  3.9789e-01,\n",
       "         2.0184e-01,  3.2833e-02,  2.6463e-02,  1.5044e-01,  4.1796e-01,\n",
       "         1.4054e-02,  7.7591e-02,  1.5288e-01,  2.2206e-01,  1.1187e-01,\n",
       "         1.7446e-01,  8.7015e-02,  1.1112e-02,  1.4947e-02,  1.0987e-01,\n",
       "         5.0519e-02,  4.5985e-02,  7.9585e-02,  1.0367e-02,  2.1848e-01,\n",
       "         2.0200e-01,  1.0667e-01,  2.6831e-01,  1.1124e-01,  3.3876e-01,\n",
       "         6.4350e-03,  1.8371e-01,  4.2667e-02,  2.2425e-01,  6.4663e-02,\n",
       "         1.2905e-01,  1.6520e-01,  2.6733e-01,  1.2384e-01,  2.4339e-01,\n",
       "         2.3918e-01,  1.5270e-01,  1.7598e-01,  1.5287e-01,  3.1732e-02,\n",
       "         3.2937e-01,  2.5363e-01,  2.4737e-01,  5.7697e-02,  8.2253e-02,\n",
       "         2.9112e-01,  1.1685e-01,  1.3283e-01,  3.2356e-03,  3.5565e-01],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "torch.cat((u, v, torch.abs(u-v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to add the logistic regression head. Complement the `forward()` method so that it concatenates, $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{|u - v|}$ and outputs three classes. You have only two lines to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward() method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(glove.size()[0],\n",
    "                                       glove.size()[1],\n",
    "                                       padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        embedded_u = self.embeddings(u)\n",
    "        embedded_v = self.embeddings(v)\n",
    "        \n",
    "        encoded_u = self.transformer_encoder(embedded_u)\n",
    "        encoded_v = self.transformer_encoder(embedded_v)\n",
    "\n",
    "        u = encoded_u.mean(dim=0)\n",
    "        v = encoded_v.mean(dim=0)\n",
    "\n",
    "        x = torch.cat((u, v, torch.abs(u-v)))\n",
    "\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the complete model and we have three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1395, 0.2053, 0.4707], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SBERT\n",
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(sbert.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training loop. You will process one sample at a time to simplify it i.e. no batch. Record the training loss.\n",
    "\n",
    "A better design would use a `Dataset` object. We will see this construct in the last laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [19:09<00:00, 229.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in tqdm(range(5)):\n",
    "    loss_train = 0\n",
    "    \n",
    "    for u, v, label in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = sbert(u,v)\n",
    "        loss = loss_fn(output, label.squeeze()) # Squeeze to remove single-dimensional target\n",
    "        loss_train += loss.item()       # accumulate loss\n",
    "        loss.backward()                 # gradient backpropagation\n",
    "        optimizer.step()                # weight updates\n",
    "    \n",
    "    avg_loss = loss_train / len(dataset)\n",
    "    ce_train_loss.append(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(ce_train_loss)),\n",
    "            ce_train_loss, c='b', marker='x')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sbert, 'sbert_mini.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_saved = torch.load('sbert_mini.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3752,  0.3699,  2.0852], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to apply the model to a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sbert(model, token_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model.embeddings(token_indices)\n",
    "        v = model.transformer_encoder(v).mean(dim=0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7015e+00, -7.3598e-01, -5.1183e-01,  7.0884e-04,  4.6708e-01,\n",
       "        -4.6991e-02, -5.4237e-01,  1.0644e-01,  8.9848e-01,  1.1055e+00,\n",
       "        -2.1277e-01,  1.3756e-01, -1.1381e+00,  1.1299e+00, -9.9853e-01,\n",
       "        -1.8522e-01,  4.3858e-01,  2.0160e+00, -1.9685e-01,  1.6530e+00,\n",
       "        -6.0021e-01, -5.6361e-01,  7.3736e-01,  6.8343e-01, -1.6816e-02,\n",
       "        -1.5255e-01,  1.1316e+00, -4.4547e-01, -1.5917e+00, -9.1902e-01,\n",
       "        -3.8385e-01, -8.6190e-01, -1.4380e+00, -5.5944e-01, -2.8237e-02,\n",
       "         1.6996e-01, -9.8841e-02,  1.0892e+00, -1.4769e-01, -9.7577e-01,\n",
       "         4.2265e-03,  5.5718e-01,  1.0991e+00,  4.6018e-01,  1.4717e-01,\n",
       "        -2.7364e-01, -8.9666e-01,  1.4678e+00,  1.5516e-01,  6.2068e-01])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sbert(sbert, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'contradiction', 1: 'entailment', 2: 'neutral'}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to all our pairs and, for each pair, we compute the cosine of the resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5493/5493 [00:56<00:00, 97.58it/s] \n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "sbert.eval()\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        encode_sbert(sbert, data[0]),\n",
    "        encode_sbert(sbert, data[1]))\n",
    "    cos_sim[idx2label[data[2].item()]] += cos_val\n",
    "    cnt[idx2label[data[2].item()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entailment': tensor(1222.3413),\n",
       "  'neutral': tensor(915.1846),\n",
       "  'contradiction': tensor(716.1481)},\n",
       " {'entailment': 1839.0, 'neutral': 1821.0, 'contradiction': 1833.0})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.6647),\n",
       " 'neutral': tensor(0.5026),\n",
       " 'contradiction': tensor(0.3907)}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Embedder to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'is', 'lovely', 'today', '.'],\n",
       " ['it', \"'\", 's', 'so', 'sunny', 'outside', '!'],\n",
       " ['he', 'drove', 'to', 'the', 'stadium', '.']]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = [tokenize(sent, pattern) for sent in sentences]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    3,  1623,    17, 11130,   376,     5]),\n",
       " tensor([  23,   60, 1537,  103, 9328,  590,  808]),\n",
       " tensor([  21, 3189,    7,    3, 1355,    5])]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents = [convert_symbols(sent, token2idx) for sent in tokenized_sents]\n",
    "indexed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.6471, -1.1520, -1.0209,  0.7083, -0.5868, -0.3048, -2.4037, -1.0729,\n",
       "          0.6895,  0.0915, -0.3880,  0.4068,  1.9630,  0.7959, -1.1882,  1.2101,\n",
       "          1.4442,  1.2408,  0.2330,  0.1286,  0.2901, -0.6119,  1.4576,  0.4293,\n",
       "         -0.5381, -0.4910,  1.3125, -0.4869, -0.5163,  0.4121, -0.4718, -0.1299,\n",
       "          0.9687, -0.4084, -0.2085,  1.2472, -0.9066, -0.8783, -0.5150, -0.6587,\n",
       "          0.0877, -0.6861,  1.2388, -0.0476, -0.2321,  0.1711, -1.1641,  0.2463,\n",
       "         -0.1327,  1.1034]),\n",
       " tensor([-0.9933, -0.5579,  0.0728,  0.4438, -0.6376, -0.3377, -1.4228, -1.2542,\n",
       "         -0.1848,  0.5228, -0.2559,  0.6364,  0.2440, -0.2104, -0.6582,  0.1255,\n",
       "          1.0311,  0.1120,  0.3396,  0.6791,  0.6286, -0.7919,  0.9081, -0.0721,\n",
       "         -0.8085, -0.7871,  0.8409,  0.2959,  0.1716,  0.3679,  0.1204,  0.9294,\n",
       "          0.7599,  0.5572, -0.2048,  0.1429, -0.3859, -0.7057,  0.2908, -1.0396,\n",
       "          0.3142,  0.2717,  0.4206, -0.2737, -0.5006,  0.1569, -0.7693, -0.3890,\n",
       "          0.5242,  1.3250]),\n",
       " tensor([-0.5117, -0.8427, -0.0382, -1.5172, -0.8918,  0.8442, -1.4458, -0.4783,\n",
       "          1.8558,  1.1138,  0.5728,  1.2191, -0.4532, -1.1174, -0.2508,  1.6857,\n",
       "         -0.2167,  0.5443,  1.1205,  1.4576,  0.0888,  0.7330,  0.5701, -0.0644,\n",
       "         -1.6095, -1.0863,  1.9768, -0.3142,  0.4220, -1.0215, -0.3662, -1.2761,\n",
       "          0.1981, -0.4561,  0.7187,  0.2300,  1.3667,  0.4614, -0.8324, -1.0910,\n",
       "         -0.0769,  0.0143,  0.2435, -0.4315, -0.7568, -0.9297,  0.0508,  0.9359,\n",
       "         -0.7172,  0.3975])]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sents = [encode_sbert(sbert, sent) for sent in indexed_sents]\n",
    "encoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6785, 0.3226],\n",
       "        [0.6785, 1.0000, 0.2801],\n",
       "        [0.3226, 0.2801, 1.0000]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([compute_cosine(s1, s2)\n",
    "              for s1 in encoded_sents\n",
    "              for s2 in encoded_sents]).reshape(len(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Write a short individual report on your program. I recommend that you use this structure for your report:\n",
    "      1. Objectives and dataset\n",
    "      2. Method and program structure, where you should outline your program and possibly describe difficult parts.\n",
    "      3. Results.\n",
    "      4. Conclusion.\n",
    "2. In Sect. _Method and program structure_, do not forget to:\n",
    "   * Summarize the baseline\n",
    "   * Summarize SBERT\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, use Latex. This will probably help you structure your text. You can use the Overleaf online editor (www.overleaf.com). You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 18, 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
